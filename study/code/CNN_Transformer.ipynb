{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg: \n",
    "    valid_size = 0.2\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed = 1212\n",
    "    pad_idx = 0\n",
    "    n_head = 8\n",
    "    ffn_hidden = 2048\n",
    "    n_layers = 4\n",
    "    max_width = 310\n",
    "    max_len = 8\n",
    "    drop_prob = 0.1\n",
    "    epochs = 20\n",
    "    learning_rate = 0.00001 # 0.0001\n",
    "    lr_min = 0.0000001\n",
    "    batch_size = 64\n",
    "    num_workers = 4 # 본인의 GPU, CPU 환경에 맞게 설정\n",
    "    print_step = 50\n",
    "    pt_path = 'model_20epoch.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Encoding 코드\n",
    "# https://github.com/ayumiymk/aster.pytorch/blob/be670046c775b54de79766208f0c59321ae1eccf/lib/models/resnet_aster.py#L37\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                   padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "class AsterBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(AsterBlock, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet_ASTER(nn.Module):\n",
    "  \"\"\"For aster or crnn\"\"\"\n",
    "\n",
    "  def __init__(self, with_lstm=False, n_group=1):\n",
    "    super(ResNet_ASTER, self).__init__()\n",
    "    self.with_lstm = with_lstm\n",
    "    self.n_group = n_group\n",
    "\n",
    "    in_channels = 3\n",
    "    self.layer0 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 32, kernel_size=(3, 3), stride=1, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "    self.inplanes = 32\n",
    "    self.layer1 = self._make_layer(32,  3, [2, 2]) # [16, 50]\n",
    "    self.layer2 = self._make_layer(64,  4, [2, 2]) # [8, 25]\n",
    "    self.layer3 = self._make_layer(128, 6, [2, 1]) # [4, 25]\n",
    "    self.layer4 = self._make_layer(256, 6, [2, 1]) # [2, 25]\n",
    "    self.layer5 = self._make_layer(512, 3, [2, 1]) # [1, 25]\n",
    "    self.layer6 = self._make_layer(512, 3, [2, 1])\n",
    "    \n",
    "    if with_lstm:\n",
    "      self.rnn = nn.LSTM(512, 256, bidirectional=True, num_layers=2, batch_first=True)\n",
    "      self.out_planes = 2 * 256\n",
    "    else:\n",
    "      self.out_planes = 512\n",
    "\n",
    "#     for m in self.modules():\n",
    "#       if isinstance(m, nn.Conv2d):\n",
    "#         nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "#       elif isinstance(m, nn.BatchNorm2d):\n",
    "#         nn.init.constant_(m.weight, 1)\n",
    "#         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "  def _make_layer(self, planes, blocks, stride):\n",
    "    downsample = None\n",
    "    if stride != [1, 1] or self.inplanes != planes:\n",
    "      downsample = nn.Sequential(\n",
    "          conv1x1(self.inplanes, planes, stride),\n",
    "          nn.BatchNorm2d(planes))\n",
    "\n",
    "    layers = []\n",
    "    layers.append(AsterBlock(self.inplanes, planes, stride, downsample))\n",
    "    self.inplanes = planes\n",
    "    for _ in range(1, blocks):\n",
    "      layers.append(AsterBlock(self.inplanes, planes))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x0 = self.layer0(x)\n",
    "    x1 = self.layer1(x0)\n",
    "    x2 = self.layer2(x1)\n",
    "    x3 = self.layer3(x2)\n",
    "    x4 = self.layer4(x3)\n",
    "    x5 = self.layer5(x4)\n",
    "    x6 = self.layer6(x5)\n",
    "    \n",
    "    cnn_feat = x6.squeeze(2) # [N, c, w]\n",
    "    cnn_feat = cnn_feat.transpose(2, 1)\n",
    "    if self.with_lstm:\n",
    "      rnn_feat, _ = self.rnn(cnn_feat)\n",
    "      return rnn_feat\n",
    "    else:\n",
    "      return cnn_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 3, 32, 100) # batch,channel,h,w\n",
    "net = ResNet_ASTER(with_lstm=True)\n",
    "encoder_feat = net(x)\n",
    "print(encoder_feat.size()) # 3,w//4,channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device): # ex. d_model = 512, max_len = 100\n",
    "        # d_model : embedding dimension\n",
    "        # max_len : 전체 데이터 문장에 대한 최대길이\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device) # self.encoding -> (max_len , d_model)\n",
    "        self.encoding.requires_grad = False  # we don't need to compute gradient (학습할 필요가 없는 값)\n",
    "\n",
    "        pos = torch.arange(0, max_len, device=device) # pos -> (max_len) # ex. pos = [0,1,2,3,...,99]\n",
    "        pos = pos.float().unsqueeze(dim=1) # pos -> (max_len, 1) \n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float() # _2i -> (d_model//2) # ex. _2i = [0,2,4,...,510]\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        # self.encoding[i,j] -> j가 짝수 : torch.sin(i/(10000)**(j/512))\n",
    "        #                    -> j가 홀수 : torch.cos(i/(10000)**((j-1)/512))\n",
    "\n",
    "        # self.encoding 은 i번째 단어에 대해 i번째 단어라는것을 구분짓기 위한 encoding 값을 제공함\n",
    "        \n",
    "    def forward(self, x): # x -> (Batch,Length)\n",
    "        if len(x.size())>=3:\n",
    "            b, seq_len , d_e = x.size() # seq_len != max_len (seq_len : 이번 배치에서의 seq_len)\n",
    "        else:\n",
    "            b, seq_len = x.size()\n",
    "\n",
    "        # seq_len이 배치내의 문장 최대 길이이므로 seq_len까지 단어 순서를 구분해주기 위한 encoding 값을 가져감\n",
    "        return self.encoding[:seq_len, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob,device): # max_len은 전체 데이터에 대한 max_len\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len,device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): # x -> (Batch,Length) (Length : Batch내의 최대 문장 길이)\n",
    "        tok_emb = self.tok_emb(x) # tok_emb -> (Batch,Length,d_model)\n",
    "        pos_emb = self.pos_emb(x) # pos_emb -> (Length,d_model)\n",
    "        return self.drop_out(tok_emb + pos_emb) # (Batch,Length,d_model)  # pos_emb가 broadcasting 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderEmbedding(nn.Module):\n",
    "    def __init__(self, max_width, drop_prob,device): # max_len은 전체 데이터에 대한 max_len\n",
    "        super(TransformerEncoderEmbedding, self).__init__()\n",
    "        self.pos_emb = PositionalEncoding(512, max_width//4+1,device) # 가장긴 이미지폭의 4분의1이기때문에\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): # x -> (Batch,Length,Embedding) (Length : Batch내의 가장긴 이미지 폭//4)\n",
    "        pos_emb = self.pos_emb(x) # pos_emb -> (Length,Embedding)\n",
    "        return self.drop_out(x + pos_emb) # (Batch,Length,Embedding)  # pos_emb가 broadcasting 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, inf_value=1e12): \n",
    "        # q,k,v -> [batch_size, head, length, d_tensor]  (d_tensor = d_model // n_head)\n",
    "        # mask -> [batch_size , 1 , len_query , len_key]\n",
    "\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        k_t = k.transpose(2, 3)  # transpose # (B,n_head,d,L_k)\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor) # (B,n_head,L_q,L_k)\n",
    "\n",
    "        if mask is not None: # (B, 1, L, L)\n",
    "            score = score.masked_fill(mask == False, (-1)*inf_value) \n",
    "            # softmax 적용시 e^(-inf) = 0이 되므로 0대신 -inf를 넣어줌 0을 넣으면 e^0 = 1 로 1이 나오게됨\n",
    "\n",
    "        score = self.softmax(score) # (B,n_head,L_q,L_k)\n",
    " \n",
    "        v = score @ v # (B,n_head,L_q,d_tensor)\n",
    "        # @ = matmul , mul은 원소별 곱셈 \n",
    "\n",
    "        return v, score  # v(attention output) -> (B,n_head,L_q,d_tensor) , score -> (B,n_head,L,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None): \n",
    "        # q,k,v -> [Batch,Length,d_model] , mask -> [batch_size , 1 , len_query , len_key]\n",
    "        \n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v) # [batch_size, length, d_model]\n",
    "        # 합쳐서 연산하나 나눠서 연산하나 Linear 적용하는 부분은 똑같기때문에 한번에 연산함.\n",
    "        \n",
    "        q, k, v = self.split(q), self.split(k), self.split(v) \n",
    "        # [batch_size, head, length, d_tensor]  (d_tensor = d_model // self.n_head)\n",
    "\n",
    "        out, attention = self.attention(q, k, v, mask=mask) \n",
    "        # v(attention output) -> (B,n_head,L_q,d_tensor) , score -> (B,n_head,L,L)\n",
    "        # v-> out , score -> attention\n",
    "\n",
    "        out = self.concat(out) # (batch_size, length, d_model)\n",
    "        out = self.w_concat(out) # (batch_size, length, d_model)\n",
    "\n",
    "        return out # (batch_size, length, d_model)\n",
    "\n",
    "    def split(self, tensor): # tensor -> [batch_size, length, d_model]\n",
    "\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor # tensor -> [batch_size, head, length, d_tensor]\n",
    "\n",
    "    def concat(self, tensor): # tensor-> [batch_size, head, length, d_tensor]\n",
    "\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor # tensor -> [batch_size, length, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # 1. compute self attention\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # 3. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "      \n",
    "        # 4. add and norm\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, max_width, ffn_hidden, n_head, n_layers, drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEncoderEmbedding(max_width=max_width,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        device=device\n",
    "                                        )\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=512,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask):    \n",
    "        # 1. compute self attention\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        if enc is not None:\n",
    "            # 3. compute encoder - decoder attention\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "            \n",
    "            # 4. add and norm\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        # 5. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # 6. add and norm\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device\n",
    "                                        )\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "\n",
    "        # pass to LM head\n",
    "        output = self.linear(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, pad_idx, dec_voc_size, n_head, max_width, max_len,\n",
    "                ffn_hidden, n_layers, drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx # 길이 맞춰주기 위한 패딩 보통 1 사용\n",
    "        self.device = device\n",
    "        self.cnn_encoder = ResNet_ASTER(with_lstm=True)\n",
    "        self.encoder = Encoder(n_head=n_head,\n",
    "                            max_width=max_width,\n",
    "                            ffn_hidden=ffn_hidden,\n",
    "                            drop_prob=drop_prob,\n",
    "                            n_layers=n_layers,\n",
    "                            device=device\n",
    "                            )\n",
    "        \n",
    "        self.decoder = Decoder(d_model=512,\n",
    "                            n_head=n_head,\n",
    "                            max_len=max_len,\n",
    "                            ffn_hidden=ffn_hidden,\n",
    "                            dec_voc_size=dec_voc_size,\n",
    "                            drop_prob=drop_prob,\n",
    "                            n_layers=n_layers,\n",
    "                            device=device\n",
    "                            )\n",
    "\n",
    "    def forward(self, src, trg, src_mask_base): # src->[Batch,channel,h,w],trg -> [Batch,Length] # src_mask->[Batch,w//4]\n",
    "        \n",
    "        src = self.cnn_encoder(src) # src -> [Batch,w//4,512]\n",
    "        \n",
    "        src_mask = self.make_pad_mask(src_mask_base, src_mask_base) # [batch_size , 1 , len_src , len_src] \n",
    "        \n",
    "        src_trg_mask = self.make_pad_mask(trg, src_mask_base) # [batch_size , 1 , len_trg , len_src] \n",
    "        \n",
    "        trg_mask = self.make_pad_mask(trg, trg) * self.make_no_peak_mask(trg, trg) # [batch_size , 1 , len_trg , len_trg]\n",
    "        # make_pad_mask(trg, trg) -> [batch_size , 1 , len_trg , len_trg]\n",
    "        # make_no_peak_mask(trg, trg) -> [len_trg , len_trg]  (broadcasting 적용)\n",
    "        enc_src = self.encoder(src, src_mask) # enc_src -> (batch_size, length, d_model)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def make_pad_mask(self, q, k): # q,k -> [Batch,Length]\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # batch_size x 1 x 1 x len_k (unsqueeze는 강제로 그 차원에 1차원을 넣어줌)\n",
    "        k = k.ne(self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1) # bx1 , 1x1 , 1xlen_1 , len_k x 1 차원이 되는것임 (repeat)\n",
    "\n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        mask = k & q # 둘다 True일경우 True 반환 나머지는 모두 False\n",
    "        return mask\n",
    "\n",
    "    def make_no_peak_mask(self, q, k): # q,k -> [Batch,Length]\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # tril 은 대각선 윗부분을 0으로 만들어주는것\n",
    "        # len_q x len_k\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "df['label_split']=df['label'].apply(lambda x : list(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 1글자 샘플들의 단어사전이 학습/테스트 데이터의 모든 글자를 담고 있으므로 학습 데이터로 우선 배치\n",
    "df['len'] = df['label'].str.len()\n",
    "train_v1 = df[df['len']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.max_len = max(df['len'])+2 # 앞뒤 패드\n",
    "print(cfg.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 2글자 이상의 샘플들에 대해서 단어길이를 고려하여 Train (80%) / Validation (20%) 분할\n",
    "df = df[df['len']>1]\n",
    "train_v2, val, _, _ = train_test_split(df, df['len'], test_size=cfg.valid_size, random_state=cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터로 우선 배치한 1글자 샘플들과 분할된 2글자 이상의 학습 샘플을 concat하여 최종 학습 데이터로 사용\n",
    "train = pd.concat([train_v1, train_v2])\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터로부터 단어 사전(Vocabulary) 구축\n",
    "train_gt = [gt for gt in train['label']]\n",
    "train_gt = \"\".join(train_gt)\n",
    "letters = sorted(list(set(list(train_gt))))\n",
    "print(len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(letters[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['pad']+[\"start\"]+['end'] + letters\n",
    "print(len(vocabulary))\n",
    "idx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
    "char2idx = {v:k for k,v in idx2char.items()}\n",
    "\n",
    "cfg.dec_voc_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx2char[0])\n",
    "print(idx2char[1])\n",
    "print(idx2char[2])\n",
    "print(idx2char[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_data']=train['label_split'].apply(lambda x :[1]+[char2idx[item] for item in x]+[2])\n",
    "val['label_data']=val['label_split'].apply(lambda x :[1]+[char2idx[item] for item in x]+[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_path_list[index]).convert('RGB')\n",
    "        \n",
    "        if self.train_mode:\n",
    "            image = self.train_transform(image)\n",
    "        else:\n",
    "            image = self.test_transform(image)\n",
    "            \n",
    "        if self.label_list is not None:\n",
    "            text = self.label_list[index]\n",
    "            return image, torch.LongTensor(text)\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    # Image Augmentation\n",
    "    def train_transform(self, image):\n",
    "        transform_ops = A.Compose([\n",
    "#             A.Resize(cfg.height,cfg.width,p=1.0),\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n",
    "            A.Rotate(limit=(-15,15), p=0.5, border_mode=cv2.BORDER_REPLICATE),\n",
    "            A.Normalize(mean=(0.8624, 0.8624, 0.8624), std=(0.19037, 0.19037, 0.19037),max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0)\n",
    "        ], p=1.0)\n",
    "        return transform_ops(image=np.array(image))['image']\n",
    "    \n",
    "    def test_transform(self, image):\n",
    "        transform_ops = A.Compose([\n",
    "#             A.Resize(cfg.height,cfg.width,p=1.0),\n",
    "            A.Normalize(mean=(0.8624, 0.8624, 0.8624), std=(0.19037, 0.19037, 0.19037),max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0)\n",
    "        ], p=1.0)\n",
    "        return transform_ops(image=np.array(image))['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    img_batch, tgt_batch = [], []\n",
    "    pad_mask_batch =[]\n",
    "    max_w = 0\n",
    "    for img, tgt_sample in batch:\n",
    "        w = img.size(2)\n",
    "        if w>max_w:\n",
    "            max_w=w        \n",
    "        tgt_batch.append(tgt_sample)\n",
    "    if max_w%4!=0:\n",
    "        max_w+=(4-max_w%4)\n",
    "    for img, _ in batch:\n",
    "        w= img.size(2)\n",
    "        new_img = torch.nn.functional.pad(img, (0,max_w-w,0,0), mode='replicate', value=0)\n",
    "        pad_mask_batch.append(max_w-w)\n",
    "        img_batch.append(new_img)\n",
    "\n",
    "    tgt_batch = pad_sequence(tgt_batch,batch_first=True, padding_value=0)\n",
    "    return torch.stack(img_batch), tgt_batch , pad_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_test(batch):\n",
    "    img_batch = []\n",
    "    pad_mask_batch =[]\n",
    "    max_w = 0\n",
    "    for img in batch:\n",
    "        w = img.size(2)\n",
    "        if w>max_w:\n",
    "            max_w=w        \n",
    "    if max_w%4!=0:\n",
    "        max_w+=(4-max_w%4)\n",
    "        \n",
    "    for img in batch:\n",
    "        w= img.size(2)\n",
    "        new_img = torch.nn.functional.pad(img, (0,max_w-w,0,0), mode='replicate', value=0)\n",
    "        pad_mask_batch.append(max_w-w)\n",
    "        img_batch.append(new_img)\n",
    "\n",
    "    return torch.stack(img_batch), pad_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['img_path'].values, train['label_data'].values, True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = cfg.batch_size, shuffle=True, num_workers=cfg.num_workers,collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label_data'].values, False)\n",
    "val_loader = DataLoader(val_dataset, batch_size = cfg.batch_size, shuffle=True, num_workers=cfg.num_workers,collate_fn=collate_fn)\n",
    "\n",
    "val_test_dataset = CustomDataset(val['img_path'].values, None, False)\n",
    "val_test_loader = DataLoader(val_test_dataset, batch_size = cfg.batch_size, shuffle=False, num_workers=cfg.num_workers,collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, text_batch ,pad_mask_batch= next(iter(train_loader))\n",
    "print(image_batch, text_batch,pad_mask_batch)\n",
    "print(image_batch.size()),print(text_batch.size()),print(len(pad_mask_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Transformer(\n",
    "    cfg.pad_idx,\n",
    "    cfg.dec_voc_size,\n",
    "    cfg.n_head,\n",
    "    cfg.max_width,\n",
    "    cfg.max_len,\n",
    "    cfg.ffn_hidden,\n",
    "    cfg.n_layers,\n",
    "    cfg.drop_prob,\n",
    "    cfg.device\n",
    ").to(cfg.device)\n",
    "\n",
    "# Adam optimizer로 학습 최적화\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = cfg.pad_idx)\n",
    "            \n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer = optimizer, T_0 = cfg.epochs, eta_min = cfg.lr_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.pt_path is not None:\n",
    "    model.load_state_dict(torch.load(cfg.pt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [2,4,1,5,7,10]\n",
    "max_len = 12\n",
    "batch_img_mask=[]\n",
    "for pad_len in a:\n",
    "    img_mask = [1 for _ in range(max_len//4-pad_len//4)]+[0 for _ in range(pad_len//4)]\n",
    "    batch_img_mask.append(img_mask)\n",
    "print(batch_img_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip,epoch, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    iters = len(iterator)\n",
    "    for i, batch in enumerate(tqdm(iterator)):\n",
    "        image = batch[0].to(device) # Batch,channel,height,width\n",
    "        text = batch[1].to(device) # Batch,Lenght\n",
    "        pad_mask = batch[2]\n",
    "        batch_w = image.size(-1)\n",
    "        batch_img_mask=[]\n",
    "        for pad_len in pad_mask:\n",
    "            img_mask = [1 for _ in range(batch_w//4-pad_len//4)]+[0 for _ in range(pad_len//4)]\n",
    "            batch_img_mask.append(img_mask)\n",
    "        batch_img_mask=torch.LongTensor(batch_img_mask).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 출력 단어의 마지막 인덱스는 제외 \n",
    "        output = model(image, text[:, :-1],batch_img_mask) # output -> [Batch, trg_len - 1, output_dim]\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1]) # output_reshape -> [Batch*(trg_len-1), output_dim]\n",
    "        text = text[:, 1:].contiguous().view(-1) # trg -> [Batch*(trg_len-1)]\n",
    "\n",
    "        loss = criterion(output_reshape, text)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        scheduler.step(epoch+i/iters)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        if i%cfg.print_step==0:\n",
    "            now_lr = get_lr(optimizer)\n",
    "            print(f'step : {i/cfg.print_step+1}, loss : {epoch_loss/(i+1)}, now_lr : {now_lr}')\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion,device):\n",
    "    model.eval() # 평가 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            image = batch[0].to(device) # Batch,channel,height,width\n",
    "            text = batch[1].to(device) # Batch,Length\n",
    "            \n",
    "            pad_mask = batch[2]\n",
    "            batch_w = image.size(-1)\n",
    "            batch_img_mask=[]\n",
    "            for pad_len in pad_mask:\n",
    "                img_mask = [1 for _ in range(batch_w//4-pad_len//4)]+[0 for _ in range(pad_len//4)]\n",
    "                batch_img_mask.append(img_mask)\n",
    "            batch_img_mask=torch.LongTensor(batch_img_mask).to(device)\n",
    "            output = model(image, text[:,:-1],batch_img_mask) # output -> [Batch, trg_len - 1, output_dim]\n",
    "\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1]) # output_reshape -> [Batch*(trg_len-1), output_dim]\n",
    "            \n",
    "            # start token 제외\n",
    "            text = text[:,1:].contiguous().view(-1) # text -> [Batch*(trg_len-1)]\n",
    "\n",
    "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "            loss = criterion(output_reshape, text)\n",
    "\n",
    "            # 전체 손실 값 계산\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator,device):\n",
    "    model.eval() # 평가 모드\n",
    "\n",
    "    with torch.no_grad():\n",
    "        answer_list=[]\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            image = batch[0].to(device) # Batch,channel,height,width\n",
    "            \n",
    "            src = model.cnn_encoder(image) # src -> [Batch,w//4,512]\n",
    "            \n",
    "            pad_mask = batch[1]\n",
    "            batch_w = image.size(-1)\n",
    "            batch_img_mask=[]\n",
    "            for pad_len in pad_mask:\n",
    "                img_mask = [1 for _ in range(batch_w//4-pad_len//4)]+[0 for _ in range(pad_len//4)]\n",
    "                batch_img_mask.append(img_mask)\n",
    "            batch_img_mask=torch.LongTensor(batch_img_mask).to(device)\n",
    "            \n",
    "            src_mask = model.make_pad_mask(batch_img_mask, batch_img_mask) \n",
    "            \n",
    "            enc_src = model.encoder(src,src_mask)\n",
    "            \n",
    "            trg_tensor = torch.ones(image.size()[0], 1).type(torch.LongTensor).to(device) # Batch,1\n",
    "            score_dict = [trg_tensor for _ in range(cfg.beem_search_k)]\n",
    "            for i in range(cfg.max_len): # 출력하고 싶은 문장의 최대길이\n",
    "                \n",
    "                src_trg_mask = model.make_pad_mask(trg_tensor, batch_img_mask) # [batch_size , 1 , len_trg , len_src] \n",
    "                trg_mask = model.make_pad_mask(trg_tensor, trg_tensor) * model.make_no_peak_mask(trg_tensor, trg_tensor) \n",
    "                \n",
    "                output = model.decoder(trg_tensor, enc_src, trg_mask, src_trg_mask)\n",
    "                \n",
    "                # 출력 문장에서 가장 마지막 단어만 사용 # trg가 한단어라면 한단어가 결과로 나옴. 4단어라면 4단어 결과가나옴.\n",
    "                pred_token = output.argmax(-1)[:,-1].unsqueeze(1) # output -> [Batch, trg_len - 1, output_dim] -> [Batch, trg_len - 1]\n",
    "                trg_tensor = torch.cat([trg_tensor,pred_token],dim=-1)\n",
    "                \n",
    "            \n",
    "            trg_tensor = trg_tensor.tolist() # [Batch,max_len]\n",
    "            # 각 출력 단어 인덱스를 실제 단어로 변환\n",
    "            word_list=[]\n",
    "            for word in trg_tensor:\n",
    "                spel_list=[]\n",
    "                for idx in word[1:]:\n",
    "                    if idx ==0 or idx == 1 or idx ==2:\n",
    "                        break\n",
    "                    char = idx2char[idx]\n",
    "                    spel_list.append(char)\n",
    "                word_list.append(\"\".join(spel_list))\n",
    "            \n",
    "            answer_list+=word_list\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.clip = 5\n",
    "best_accuracy = 0\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, cfg.clip, epoch, cfg.device)\n",
    "    valid_loss = evaluate(model, val_loader, criterion, cfg.device)\n",
    "\n",
    "    answer_list = test(model, val_test_loader, cfg.device)\n",
    "    accuracy = accuracy_score(val['label'],answer_list)\n",
    "\n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'model_best_accuracy.pt')\n",
    "    \n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model_best_loss.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f}')\n",
    "    print(f'\\tAccuracy : {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_best_loss.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df= pd.read_csv('./test.csv')\n",
    "\n",
    "test_dataset = CustomDataset(test_df['img_path'].values, None, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = cfg.batch_size, shuffle=False, num_workers=cfg.num_workers,collate_fn=collate_fn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_list = test(model, test_loader, cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['label'] = answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./cnn_transformer_config1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
