{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56228f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.13.1\n",
      "torchvision version: 0.14.1\n",
      "torchtext version: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchtext\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f'torch version: {torch.__version__}')\n",
    "print(f'torchvision version: {torchvision.__version__}')\n",
    "print(f'torchtext version: {torchtext.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b94486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ec486d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download de\n",
    "# !pip install torchtext==0.6.0\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1217a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load(\"en_core_web_sm\") # 영어 토큰화(tokenization)\n",
    "spacy_de = spacy.load(\"de_core_news_sm\") # 독일어 토큰화(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c59d2293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 0: I\n",
      "인덱스 1: am\n",
      "인덱스 2: a\n",
      "인덱스 3: graduate\n",
      "인덱스 4: student\n",
      "인덱스 5: .\n"
     ]
    }
   ],
   "source": [
    "# 간단히 토큰화(tokenization) 기능 써보기\n",
    "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
    "\n",
    "for i, token in enumerate(tokenized):\n",
    "    print(f\"인덱스 {i}: {token.text}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6bb657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 독일어(Deutsch) 문장을 토큰화 하는 함수 (순서를 뒤집지 않음)\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "# 영어(English) 문장을 토큰화 하는 함수\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f29fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필드(Fielld) 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시.\n",
    "# Seq2Seq 모델과 다르게 batch_first 속성의 값을 True 로 설정\n",
    "# 번역 목표 : 소스(SRC)독일어 -> 목표(TRG)영어\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b16e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c216653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋(training dataset) 크기: 29000개\n",
      "평가 데이터셋(validation dataset) 크기: 1014개\n",
      "테스트 데이터셋(testing dataset) 크기: 1000개\n"
     ]
    }
   ],
   "source": [
    "print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\")\n",
    "print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\")\n",
    "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42a67280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ein', 'mann', ',', 'der', 'mit', 'einer', 'tasse', 'kaffee', 'an', 'einem', 'urinal', 'steht', '.']\n",
      "['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 중 하나를 선택해 출력\n",
    "print(vars(train_dataset.examples[30])['src'])\n",
    "print(vars(train_dataset.examples[30])['trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe9ef2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(SRC): 7853\n",
      "len(TRG): 5893\n"
     ]
    }
   ],
   "source": [
    "# 필드 객체인 build_vocab 메서드를 이용해 영어와 독어의 단어 사전을 생성 (최소 2번 이상 등장한 단어만을 선택)\n",
    "\n",
    "SRC.build_vocab(train_dataset, min_freq=2)\n",
    "TRG.build_vocab(train_dataset, min_freq=2)\n",
    "\n",
    "print(f\"len(SRC): {len(SRC.vocab)}\")\n",
    "print(f\"len(TRG): {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b67397d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4112\n",
      "1752\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n",
    "print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n",
    "print(TRG.vocab.stoi[\"<sos>\"]) # : 2 -> start token\n",
    "print(TRG.vocab.stoi[\"<eos>\"]) # : 3 -> end token\n",
    "print(TRG.vocab.stoi[\"hello\"])\n",
    "print(TRG.vocab.stoi[\"world\"])\n",
    "\n",
    "print(SRC.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n",
    "print(SRC.vocab.stoi[SRC.pad_token])\n",
    "print(SRC.vocab.stoi[\"<sos>\"]) # : 2 -> start token\n",
    "print(SRC.vocab.stoi[\"<eos>\"]) # : 3 -> end token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2676254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장에 포함된 단어가 순서대로 나열된 상태로 네트워크에 입력되어야 함\n",
    "# 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋음 -> 이를 위해 BucketIterator 사용\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset),\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fb78a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치 크기: torch.Size([128, 27])\n",
      "인덱스 0: 2\n",
      "인덱스 1: 18\n",
      "인덱스 2: 0\n",
      "인덱스 3: 858\n",
      "인덱스 4: 9\n",
      "인덱스 5: 5\n",
      "인덱스 6: 13\n",
      "인덱스 7: 10\n",
      "인덱스 8: 8\n",
      "인덱스 9: 16\n",
      "인덱스 10: 9\n",
      "인덱스 11: 1028\n",
      "인덱스 12: 12\n",
      "인덱스 13: 15\n",
      "인덱스 14: 136\n",
      "인덱스 15: 8\n",
      "인덱스 16: 2088\n",
      "인덱스 17: 4\n",
      "인덱스 18: 3\n",
      "인덱스 19: 1\n",
      "인덱스 20: 1\n",
      "인덱스 21: 1\n",
      "인덱스 22: 1\n",
      "인덱스 23: 1\n",
      "인덱스 24: 1\n",
      "인덱스 25: 1\n",
      "인덱스 26: 1\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "\n",
    "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
    "\n",
    "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
    "    for i in range(src.shape[1]):\n",
    "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\n",
    "\n",
    "    # 첫 번째 배치만 확인\n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd56b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device): # ex. d_model = 512, max_len = 100\n",
    "        # d_model : embedding dimension\n",
    "        # max_len : 전체 데이터 문장에 대한 최대길이\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device) # self.encoding -> (max_len , d_model)\n",
    "        self.encoding.requires_grad = False  # we don't need to compute gradient (학습할 필요가 없는 값)\n",
    "\n",
    "        pos = torch.arange(0, max_len, device=device) # pos -> (max_len) # ex. pos = [0,1,2,3,...,99]\n",
    "        pos = pos.float().unsqueeze(dim=1) # pos -> (max_len, 1) \n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float() # _2i -> (d_model//2) # ex. _2i = [0,2,4,...,510]\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        # self.encoding[i,j] -> j가 짝수 : torch.sin(i/(10000)**(j/512))\n",
    "        #                    -> j가 홀수 : torch.cos(i/(10000)**((j-1)/512))\n",
    "\n",
    "        # self.encoding 은 i번째 단어에 대해 i번째 단어라는것을 구분짓기 위한 encoding 값을 제공함\n",
    "        \n",
    "    def forward(self, x): # x -> (Batch,Length)\n",
    "\n",
    "        batch_size, seq_len = x.size() # seq_len != max_len (seq_len : 이번 배치에서의 seq_len)\n",
    "\n",
    "        # seq_len이 배치내의 문장 최대 길이이므로 seq_len까지 단어 순서를 구분해주기 위한 encoding 값을 가져감\n",
    "        return self.encoding[:seq_len, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "271fa7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob,device): # max_len은 전체 데이터에 대한 max_len\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len,device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): # x -> (Batch,Length) (Length : Batch내의 최대 문장 길이)\n",
    "        tok_emb = self.tok_emb(x) # tok_emb -> (Batch,Length,d_model)\n",
    "        pos_emb = self.pos_emb(x) # pos_emb -> (Length,d_model)\n",
    "        return self.drop_out(tok_emb + pos_emb) # (Batch,Length,d_model)  # pos_emb가 broadcasting 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a3bf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, inf_value=1e12): \n",
    "        # q,k,v -> [batch_size, head, length, d_tensor]  (d_tensor = d_model // n_head)\n",
    "        # mask -> [batch_size , 1 , len_query , len_key]\n",
    "\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        k_t = k.transpose(2, 3)  # transpose # (B,n_head,d,L_k)\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor) # (B,n_head,L_q,L_k)\n",
    "\n",
    "        if mask is not None: # (B, 1, L, L)\n",
    "            score = score.masked_fill(mask == False, (-1)*inf_value) \n",
    "            # softmax 적용시 e^(-inf) = 0이 되므로 0대신 -inf를 넣어줌 0을 넣으면 e^0 = 1 로 1이 나오게됨\n",
    "\n",
    "        score = self.softmax(score) # (B,n_head,L_q,L_k)\n",
    " \n",
    "        v = score @ v # (B,n_head,L_q,d_tensor)\n",
    "        # @ = matmul , mul은 원소별 곱셈 \n",
    "\n",
    "        return v, score  # v(attention output) -> (B,n_head,L_q,d_tensor) , score -> (B,n_head,L,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9892edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None): \n",
    "        # q,k,v -> [Batch,Length,d_model] , mask -> [batch_size , 1 , len_query , len_key]\n",
    "        \n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v) # [batch_size, length, d_model]\n",
    "        # 합쳐서 연산하나 나눠서 연산하나 Linear 적용하는 부분은 똑같기때문에 한번에 연산함.\n",
    "        \n",
    "        q, k, v = self.split(q), self.split(k), self.split(v) \n",
    "        # [batch_size, head, length, d_tensor]  (d_tensor = d_model // self.n_head)\n",
    "\n",
    "        out, attention = self.attention(q, k, v, mask=mask) \n",
    "        # v(attention output) -> (B,n_head,L_q,d_tensor) , score -> (B,n_head,L,L)\n",
    "        # v-> out , score -> attention\n",
    "\n",
    "        out = self.concat(out) # (batch_size, length, d_model)\n",
    "        out = self.w_concat(out) # (batch_size, length, d_model)\n",
    "\n",
    "        return out # (batch_size, length, d_model)\n",
    "\n",
    "    def split(self, tensor): # tensor -> [batch_size, length, d_model]\n",
    "\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor # tensor -> [batch_size, head, length, d_tensor]\n",
    "\n",
    "    def concat(self, tensor): # tensor-> [batch_size, head, length, d_tensor]\n",
    "\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor # tensor -> [batch_size, length, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b8a15a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "879dc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # 1. compute self attention\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # 3. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "      \n",
    "        # 4. add and norm\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a6d35ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=enc_voc_size,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        device=device\n",
    "                                        )\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a5e190bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask):    \n",
    "        # 1. compute self attention\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        if enc is not None:\n",
    "            # 3. compute encoder - decoder attention\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "            \n",
    "            # 4. add and norm\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        # 5. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # 6. add and norm\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2306daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device\n",
    "                                        )\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "\n",
    "        # pass to LM head\n",
    "        output = self.linear(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea2bf5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, pad_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
    "                ffn_hidden, n_layers, drop_prob,device):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx # 길이 맞춰주기 위한 패딩 보통 1 사용\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                            n_head=n_head,\n",
    "                            max_len=max_len,\n",
    "                            ffn_hidden=ffn_hidden,\n",
    "                            enc_voc_size=enc_voc_size,\n",
    "                            drop_prob=drop_prob,\n",
    "                            n_layers=n_layers,\n",
    "                            device=device\n",
    "                            )\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                            n_head=n_head,\n",
    "                            max_len=max_len,\n",
    "                            ffn_hidden=ffn_hidden,\n",
    "                            dec_voc_size=dec_voc_size,\n",
    "                            drop_prob=drop_prob,\n",
    "                            n_layers=n_layers,\n",
    "                            device=device\n",
    "                            )\n",
    "\n",
    "    def forward(self, src, trg): # src,trg -> [Batch,Length]\n",
    "        src_mask = self.make_pad_mask(src, src) # [batch_size , 1 , len_src , len_src] \n",
    "        \n",
    "        src_trg_mask = self.make_pad_mask(trg, src) # [batch_size , 1 , len_trg , len_src] \n",
    "        \n",
    "        trg_mask = self.make_pad_mask(trg, trg) * self.make_no_peak_mask(trg, trg) # [batch_size , 1 , len_trg , len_trg]\n",
    "        # make_pad_mask(trg, trg) -> [batch_size , 1 , len_trg , len_trg]\n",
    "        # make_no_peak_mask(trg, trg) -> [len_trg , len_trg]  (broadcasting 적용)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask) # enc_src -> (batch_size, length, d_model)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def make_pad_mask(self, q, k): # q,k -> [Batch,Length]\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # 참고 : https://github.com/kyungmin1212/Data_Science/blob/main/study/pytorch_code.md#5\n",
    "\n",
    "        # batch_size x 1 x 1 x len_k (unsqueeze는 강제로 그 차원에 1차원을 넣어줌)\n",
    "        k = k.ne(self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1) # bx1 , 1x1 , 1xlen_1 , len_k x 1 차원이 되는것임 (repeat)\n",
    "\n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        mask = k & q # 둘다 True일경우 True 반환 나머지는 모두 False\n",
    "        return mask\n",
    "\n",
    "    def make_no_peak_mask(self, q, k): # q,k -> [Batch,Length]\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # tril 은 대각선 윗부분을 0으로 만들어주는것\n",
    "        # len_q x len_k\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a1ec6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서는 xavier_uniform_이 더 빠르게 수렴함. (모델 초기화는 상황에 따라 다르므로 다 해보고 좋은것을 선택)\n",
    "# def initialize_weights(m):\n",
    "#     # convolution kernel의 weight를 He initialization을 적용한다.\n",
    "#     if isinstance(m, nn.Conv2d):\n",
    "#         nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "\n",
    "#         # bias는 상수 0으로 초기화 한다.\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     elif isinstance(m, nn.BatchNorm2d):\n",
    "#         nn.init.constant_(m.weight, 1)\n",
    "#         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     elif isinstance(m, nn.Linear):\n",
    "#         nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "\n",
    "#         # bias는 상수 0으로 초기화 한다.\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "04722e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    # convolution kernel의 weight를 He initialization을 적용한다.\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "        # bias는 상수 0으로 초기화 한다.\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "        # bias는 상수 0으로 초기화 한다.\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# def initialize_weights_base(m):\n",
    "#     if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "## initialize_weights_base\n",
    "# Epoch: 10 | Time: 0m 8s \n",
    "# \tTrain Loss: 2.251 | Train PPL: 9.498\n",
    "# \tValidation Loss: 2.151 | Validation PPL: 8.590\n",
    "\n",
    "## initialize_weights\n",
    "# Epoch: 10 | Time: 0m 8s\n",
    "# \tTrain Loss: 1.565 | Train PPL: 4.781\n",
    "# \tValidation Loss: 1.723 | Validation PPL: 5.601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "152afb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2b41b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "enc_voc_size = len(SRC.vocab)\n",
    "dec_voc_size = len(TRG.vocab)\n",
    "\n",
    "batch_size = 128\n",
    "max_len = 256 # 임의로 100이라고 지정(동작 안하면 더크게 만들면 됨)\n",
    "d_model = 256\n",
    "n_layers = 3\n",
    "n_heads = 8\n",
    "ffn_hidden = 512\n",
    "drop_prob = 0.1\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token] # 1\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token] # 1\n",
    "print(SRC_PAD_IDX)\n",
    "# 없는단어 0 , 패딩 1 , START 2 , END 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "809f2fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "cb98ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(SRC_PAD_IDX, enc_voc_size, dec_voc_size, d_model, n_heads, max_len, ffn_hidden, n_layers, drop_prob,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f60b2326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): Embedding(7853, 256)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): Embedding(5893, 256)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (w_concat): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "248e4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Adam optimizer로 학습 최적화\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = SRC_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0a23848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src # Batch,Length\n",
    "        trg = batch.trg # Batch,Length\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 출력 단어의 마지막 인덱스는 제외 \n",
    "        output = model(src, trg[:, :-1]) # output -> [Batch, trg_len - 1, output_dim]\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1]) # output_reshape -> [Batch*(trg_len-1), output_dim]\n",
    "        trg = trg[:, 1:].contiguous().view(-1) # trg -> [Batch*(trg_len-1)]\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0dd57f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval() # 평가 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src # Batch,Length\n",
    "            trg = batch.trg # Batch,Length\n",
    "\n",
    "            output = model(src, trg[:,:-1]) # output -> [Batch, trg_len - 1, output_dim]\n",
    "\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1]) # output_reshape -> [Batch*(trg_len-1), output_dim]\n",
    "            \n",
    "            # start token 제외\n",
    "            trg = trg[:,1:].contiguous().view(-1) # trg -> [Batch*(trg_len-1)]\n",
    "\n",
    "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "            loss = criterion(output_reshape, trg)\n",
    "\n",
    "            # 전체 손실 값 계산\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c31d7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "eca4d89b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0.0 % , loss : 8.801682472229004\n",
      "step : 0.44 % , loss : 8.295473098754883\n",
      "step : 0.88 % , loss : 8.074953079223633\n",
      "step : 1.32 % , loss : 7.89055871963501\n",
      "step : 1.76 % , loss : 7.775366306304932\n",
      "step : 2.2 % , loss : 7.650823593139648\n",
      "step : 2.64 % , loss : 7.541178226470947\n",
      "step : 3.08 % , loss : 7.315464019775391\n",
      "step : 3.52 % , loss : 7.229629039764404\n",
      "step : 3.96 % , loss : 7.145270824432373\n",
      "step : 4.41 % , loss : 7.002967357635498\n",
      "step : 4.85 % , loss : 6.839483737945557\n",
      "step : 5.29 % , loss : 6.7777862548828125\n",
      "step : 5.73 % , loss : 6.670000076293945\n",
      "step : 6.17 % , loss : 6.5554680824279785\n",
      "step : 6.61 % , loss : 6.426968097686768\n",
      "step : 7.05 % , loss : 6.329578876495361\n",
      "step : 7.49 % , loss : 6.259642124176025\n",
      "step : 7.93 % , loss : 6.123231410980225\n",
      "step : 8.37 % , loss : 6.055042743682861\n",
      "step : 8.81 % , loss : 5.967118740081787\n",
      "step : 9.25 % , loss : 5.815183162689209\n",
      "step : 9.69 % , loss : 5.829502582550049\n",
      "step : 10.13 % , loss : 5.724875450134277\n",
      "step : 10.57 % , loss : 5.688485622406006\n",
      "step : 11.01 % , loss : 5.611427307128906\n",
      "step : 11.45 % , loss : 5.526515960693359\n",
      "step : 11.89 % , loss : 5.5214643478393555\n",
      "step : 12.33 % , loss : 5.345703601837158\n",
      "step : 12.78 % , loss : 5.384003162384033\n",
      "step : 13.22 % , loss : 5.330953598022461\n",
      "step : 13.66 % , loss : 5.369636058807373\n",
      "step : 14.1 % , loss : 5.268533706665039\n",
      "step : 14.54 % , loss : 5.272759914398193\n",
      "step : 14.98 % , loss : 5.252830505371094\n",
      "step : 15.42 % , loss : 5.1477952003479\n",
      "step : 15.86 % , loss : 5.242513656616211\n",
      "step : 16.3 % , loss : 5.148094177246094\n",
      "step : 16.74 % , loss : 5.088629722595215\n",
      "step : 17.18 % , loss : 5.089696407318115\n",
      "step : 17.62 % , loss : 5.089004039764404\n",
      "step : 18.06 % , loss : 5.036416053771973\n",
      "step : 18.5 % , loss : 4.993297100067139\n",
      "step : 18.94 % , loss : 4.950736045837402\n",
      "step : 19.38 % , loss : 4.971726417541504\n",
      "step : 19.82 % , loss : 5.0279860496521\n",
      "step : 20.26 % , loss : 4.865384101867676\n",
      "step : 20.7 % , loss : 4.973019123077393\n",
      "step : 21.15 % , loss : 4.932130813598633\n",
      "step : 21.59 % , loss : 4.877194404602051\n",
      "step : 22.03 % , loss : 4.914245128631592\n",
      "step : 22.47 % , loss : 4.7409234046936035\n",
      "step : 22.91 % , loss : 4.93862247467041\n",
      "step : 23.35 % , loss : 4.8718461990356445\n",
      "step : 23.79 % , loss : 4.668713092803955\n",
      "step : 24.23 % , loss : 4.85929536819458\n",
      "step : 24.67 % , loss : 4.784330368041992\n",
      "step : 25.11 % , loss : 4.800443172454834\n",
      "step : 25.55 % , loss : 4.636538982391357\n",
      "step : 25.99 % , loss : 4.695922374725342\n",
      "step : 26.43 % , loss : 4.741127967834473\n",
      "step : 26.87 % , loss : 4.736616134643555\n",
      "step : 27.31 % , loss : 4.635034084320068\n",
      "step : 27.75 % , loss : 4.6546196937561035\n",
      "step : 28.19 % , loss : 4.700624942779541\n",
      "step : 28.63 % , loss : 4.666788101196289\n",
      "step : 29.07 % , loss : 4.582859992980957\n",
      "step : 29.52 % , loss : 4.6107964515686035\n",
      "step : 29.96 % , loss : 4.6037750244140625\n",
      "step : 30.4 % , loss : 4.62396764755249\n",
      "step : 30.84 % , loss : 4.542104244232178\n",
      "step : 31.28 % , loss : 4.532483100891113\n",
      "step : 31.72 % , loss : 4.48427152633667\n",
      "step : 32.16 % , loss : 4.457761764526367\n",
      "step : 32.6 % , loss : 4.502592086791992\n",
      "step : 33.04 % , loss : 4.525582790374756\n",
      "step : 33.48 % , loss : 4.605809688568115\n",
      "step : 33.92 % , loss : 4.556541442871094\n",
      "step : 34.36 % , loss : 4.390280723571777\n",
      "step : 34.8 % , loss : 4.398173809051514\n",
      "step : 35.24 % , loss : 4.371730327606201\n",
      "step : 35.68 % , loss : 4.4957404136657715\n",
      "step : 36.12 % , loss : 4.430095195770264\n",
      "step : 36.56 % , loss : 4.422276973724365\n",
      "step : 37.0 % , loss : 4.405160427093506\n",
      "step : 37.44 % , loss : 4.228387832641602\n",
      "step : 37.89 % , loss : 4.396580219268799\n",
      "step : 38.33 % , loss : 4.239138126373291\n",
      "step : 38.77 % , loss : 4.533960819244385\n",
      "step : 39.21 % , loss : 4.334588050842285\n",
      "step : 39.65 % , loss : 4.3394060134887695\n",
      "step : 40.09 % , loss : 4.238867282867432\n",
      "step : 40.53 % , loss : 4.4177985191345215\n",
      "step : 40.97 % , loss : 4.243807315826416\n",
      "step : 41.41 % , loss : 4.347694396972656\n",
      "step : 41.85 % , loss : 4.201058387756348\n",
      "step : 42.29 % , loss : 4.4243927001953125\n",
      "step : 42.73 % , loss : 4.328080177307129\n",
      "step : 43.17 % , loss : 4.174982070922852\n",
      "step : 43.61 % , loss : 4.136002540588379\n",
      "step : 44.05 % , loss : 4.248195171356201\n",
      "step : 44.49 % , loss : 4.198272705078125\n",
      "step : 44.93 % , loss : 4.23463249206543\n",
      "step : 45.37 % , loss : 4.152964115142822\n",
      "step : 45.81 % , loss : 4.3575439453125\n",
      "step : 46.26 % , loss : 4.08013916015625\n",
      "step : 46.7 % , loss : 4.276723861694336\n",
      "step : 47.14 % , loss : 4.218427658081055\n",
      "step : 47.58 % , loss : 4.075403213500977\n",
      "step : 48.02 % , loss : 4.192941188812256\n",
      "step : 48.46 % , loss : 4.237493991851807\n",
      "step : 48.9 % , loss : 4.1626763343811035\n",
      "step : 49.34 % , loss : 4.3047709465026855\n",
      "step : 49.78 % , loss : 4.164614200592041\n",
      "step : 50.22 % , loss : 4.216598033905029\n",
      "step : 50.66 % , loss : 4.109933853149414\n",
      "step : 51.1 % , loss : 4.164164066314697\n",
      "step : 51.54 % , loss : 4.058710098266602\n",
      "step : 51.98 % , loss : 4.108521461486816\n",
      "step : 52.42 % , loss : 4.012961387634277\n",
      "step : 52.86 % , loss : 4.087332725524902\n",
      "step : 53.3 % , loss : 4.169508934020996\n",
      "step : 53.74 % , loss : 4.242020606994629\n",
      "step : 54.19 % , loss : 4.043724060058594\n",
      "step : 54.63 % , loss : 4.21895170211792\n",
      "step : 55.07 % , loss : 4.114058494567871\n",
      "step : 55.51 % , loss : 4.030722141265869\n",
      "step : 55.95 % , loss : 4.151895523071289\n",
      "step : 56.39 % , loss : 4.102790355682373\n",
      "step : 56.83 % , loss : 4.112539768218994\n",
      "step : 57.27 % , loss : 4.0933451652526855\n",
      "step : 57.71 % , loss : 3.8572232723236084\n",
      "step : 58.15 % , loss : 4.118884086608887\n",
      "step : 58.59 % , loss : 3.996830463409424\n",
      "step : 59.03 % , loss : 4.143621444702148\n",
      "step : 59.47 % , loss : 4.048806667327881\n",
      "step : 59.91 % , loss : 4.1044769287109375\n",
      "step : 60.35 % , loss : 4.025254726409912\n",
      "step : 60.79 % , loss : 4.074278354644775\n",
      "step : 61.23 % , loss : 4.107334136962891\n",
      "step : 61.67 % , loss : 4.073531150817871\n",
      "step : 62.11 % , loss : 4.000400066375732\n",
      "step : 62.56 % , loss : 4.095684051513672\n",
      "step : 63.0 % , loss : 4.134893417358398\n",
      "step : 63.44 % , loss : 3.9388298988342285\n",
      "step : 63.88 % , loss : 4.032232761383057\n",
      "step : 64.32 % , loss : 4.019505977630615\n",
      "step : 64.76 % , loss : 3.9917843341827393\n",
      "step : 65.2 % , loss : 4.014374732971191\n",
      "step : 65.64 % , loss : 3.920789957046509\n",
      "step : 66.08 % , loss : 3.999298572540283\n",
      "step : 66.52 % , loss : 4.079665660858154\n",
      "step : 66.96 % , loss : 4.05079460144043\n",
      "step : 67.4 % , loss : 3.9767637252807617\n",
      "step : 67.84 % , loss : 3.976168632507324\n",
      "step : 68.28 % , loss : 4.063766956329346\n",
      "step : 68.72 % , loss : 3.985731840133667\n",
      "step : 69.16 % , loss : 3.9262914657592773\n",
      "step : 69.6 % , loss : 3.774641990661621\n",
      "step : 70.04 % , loss : 3.8957884311676025\n",
      "step : 70.48 % , loss : 3.9244256019592285\n",
      "step : 70.93 % , loss : 3.847425699234009\n",
      "step : 71.37 % , loss : 3.9937474727630615\n",
      "step : 71.81 % , loss : 3.7746174335479736\n",
      "step : 72.25 % , loss : 3.833894729614258\n",
      "step : 72.69 % , loss : 3.919412612915039\n",
      "step : 73.13 % , loss : 4.013485908508301\n",
      "step : 73.57 % , loss : 3.764033317565918\n",
      "step : 74.01 % , loss : 3.967979907989502\n",
      "step : 74.45 % , loss : 3.824632406234741\n",
      "step : 74.89 % , loss : 4.000565528869629\n",
      "step : 75.33 % , loss : 3.9971067905426025\n",
      "step : 75.77 % , loss : 3.8824563026428223\n",
      "step : 76.21 % , loss : 3.931349754333496\n",
      "step : 76.65 % , loss : 4.018159866333008\n",
      "step : 77.09 % , loss : 3.86057710647583\n",
      "step : 77.53 % , loss : 3.885215997695923\n",
      "step : 77.97 % , loss : 3.891127109527588\n",
      "step : 78.41 % , loss : 3.786747455596924\n",
      "step : 78.85 % , loss : 3.7959704399108887\n",
      "step : 79.3 % , loss : 3.7648801803588867\n",
      "step : 79.74 % , loss : 3.703099012374878\n",
      "step : 80.18 % , loss : 3.710029125213623\n",
      "step : 80.62 % , loss : 3.767570972442627\n",
      "step : 81.06 % , loss : 3.8368539810180664\n",
      "step : 81.5 % , loss : 3.8814518451690674\n",
      "step : 81.94 % , loss : 3.5792338848114014\n",
      "step : 82.38 % , loss : 3.887998104095459\n",
      "step : 82.82 % , loss : 3.801107406616211\n",
      "step : 83.26 % , loss : 3.859467029571533\n",
      "step : 83.7 % , loss : 3.9017062187194824\n",
      "step : 84.14 % , loss : 3.906731128692627\n",
      "step : 84.58 % , loss : 3.853947162628174\n",
      "step : 85.02 % , loss : 3.735316514968872\n",
      "step : 85.46 % , loss : 3.8980624675750732\n",
      "step : 85.9 % , loss : 3.709059238433838\n",
      "step : 86.34 % , loss : 3.7990729808807373\n",
      "step : 86.78 % , loss : 3.901003122329712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 87.22 % , loss : 3.789759635925293\n",
      "step : 87.67 % , loss : 3.76476788520813\n",
      "step : 88.11 % , loss : 3.740016222000122\n",
      "step : 88.55 % , loss : 3.762610912322998\n",
      "step : 88.99 % , loss : 3.682814359664917\n",
      "step : 89.43 % , loss : 3.7117550373077393\n",
      "step : 89.87 % , loss : 3.7508819103240967\n",
      "step : 90.31 % , loss : 3.7757744789123535\n",
      "step : 90.75 % , loss : 3.776050090789795\n",
      "step : 91.19 % , loss : 3.5930793285369873\n",
      "step : 91.63 % , loss : 3.739190101623535\n",
      "step : 92.07 % , loss : 3.6405460834503174\n",
      "step : 92.51 % , loss : 3.874835252761841\n",
      "step : 92.95 % , loss : 3.641096830368042\n",
      "step : 93.39 % , loss : 3.7318990230560303\n",
      "step : 93.83 % , loss : 3.83461594581604\n",
      "step : 94.27 % , loss : 3.726825475692749\n",
      "step : 94.71 % , loss : 3.857269287109375\n",
      "step : 95.15 % , loss : 3.8498497009277344\n",
      "step : 95.59 % , loss : 3.5008888244628906\n",
      "step : 96.04 % , loss : 3.5836448669433594\n",
      "step : 96.48 % , loss : 3.7650539875030518\n",
      "step : 96.92 % , loss : 3.7191152572631836\n",
      "step : 97.36 % , loss : 3.7271170616149902\n",
      "step : 97.8 % , loss : 3.6598331928253174\n",
      "step : 98.24 % , loss : 3.7716991901397705\n",
      "step : 98.68 % , loss : 3.594815731048584\n",
      "step : 99.12 % , loss : 3.7421653270721436\n",
      "step : 99.56 % , loss : 3.6443898677825928\n",
      "Epoch: 01 | Time: 0m 8s\n",
      "\tTrain Loss: 4.524 | Train PPL: 92.228\n",
      "\tValidation Loss: 3.458 | Validation PPL: 31.761\n",
      "step : 0.0 % , loss : 3.623384475708008\n",
      "step : 0.44 % , loss : 3.536916732788086\n",
      "step : 0.88 % , loss : 3.64341402053833\n",
      "step : 1.32 % , loss : 3.5757715702056885\n",
      "step : 1.76 % , loss : 3.6241259574890137\n",
      "step : 2.2 % , loss : 3.5876193046569824\n",
      "step : 2.64 % , loss : 3.519334316253662\n",
      "step : 3.08 % , loss : 3.6557180881500244\n",
      "step : 3.52 % , loss : 3.668318271636963\n",
      "step : 3.96 % , loss : 3.622687339782715\n",
      "step : 4.41 % , loss : 3.5874664783477783\n",
      "step : 4.85 % , loss : 3.5792088508605957\n",
      "step : 5.29 % , loss : 3.5110514163970947\n",
      "step : 5.73 % , loss : 3.5214922428131104\n",
      "step : 6.17 % , loss : 3.6722166538238525\n",
      "step : 6.61 % , loss : 3.724045515060425\n",
      "step : 7.05 % , loss : 3.59010910987854\n",
      "step : 7.49 % , loss : 3.6769821643829346\n",
      "step : 7.93 % , loss : 3.3677303791046143\n",
      "step : 8.37 % , loss : 3.575045347213745\n",
      "step : 8.81 % , loss : 3.636522054672241\n",
      "step : 9.25 % , loss : 3.8160598278045654\n",
      "step : 9.69 % , loss : 3.5948688983917236\n",
      "step : 10.13 % , loss : 3.635775089263916\n",
      "step : 10.57 % , loss : 3.4570541381835938\n",
      "step : 11.01 % , loss : 3.575748920440674\n",
      "step : 11.45 % , loss : 3.604612112045288\n",
      "step : 11.89 % , loss : 3.385089635848999\n",
      "step : 12.33 % , loss : 3.548705577850342\n",
      "step : 12.78 % , loss : 3.5592737197875977\n",
      "step : 13.22 % , loss : 3.5702643394470215\n",
      "step : 13.66 % , loss : 3.6157889366149902\n",
      "step : 14.1 % , loss : 3.522639036178589\n",
      "step : 14.54 % , loss : 3.5132367610931396\n",
      "step : 14.98 % , loss : 3.4805145263671875\n",
      "step : 15.42 % , loss : 3.6531636714935303\n",
      "step : 15.86 % , loss : 3.511955738067627\n",
      "step : 16.3 % , loss : 3.570103883743286\n",
      "step : 16.74 % , loss : 3.6294302940368652\n",
      "step : 17.18 % , loss : 3.49434232711792\n",
      "step : 17.62 % , loss : 3.6965365409851074\n",
      "step : 18.06 % , loss : 3.591256856918335\n",
      "step : 18.5 % , loss : 3.5334229469299316\n",
      "step : 18.94 % , loss : 3.635288715362549\n",
      "step : 19.38 % , loss : 3.6065502166748047\n",
      "step : 19.82 % , loss : 3.619771957397461\n",
      "step : 20.26 % , loss : 3.5049190521240234\n",
      "step : 20.7 % , loss : 3.5954384803771973\n",
      "step : 21.15 % , loss : 3.5815327167510986\n",
      "step : 21.59 % , loss : 3.749626874923706\n",
      "step : 22.03 % , loss : 3.5788042545318604\n",
      "step : 22.47 % , loss : 3.6107611656188965\n",
      "step : 22.91 % , loss : 3.660095691680908\n",
      "step : 23.35 % , loss : 3.363196611404419\n",
      "step : 23.79 % , loss : 3.5693681240081787\n",
      "step : 24.23 % , loss : 3.4575235843658447\n",
      "step : 24.67 % , loss : 3.5038070678710938\n",
      "step : 25.11 % , loss : 3.502751588821411\n",
      "step : 25.55 % , loss : 3.411511182785034\n",
      "step : 25.99 % , loss : 3.4078845977783203\n",
      "step : 26.43 % , loss : 3.385279655456543\n",
      "step : 26.87 % , loss : 3.5733211040496826\n",
      "step : 27.31 % , loss : 3.484623670578003\n",
      "step : 27.75 % , loss : 3.4442808628082275\n",
      "step : 28.19 % , loss : 3.5809900760650635\n",
      "step : 28.63 % , loss : 3.3808367252349854\n",
      "step : 29.07 % , loss : 3.4403347969055176\n",
      "step : 29.52 % , loss : 3.4632863998413086\n",
      "step : 29.96 % , loss : 3.506582260131836\n",
      "step : 30.4 % , loss : 3.481302261352539\n",
      "step : 30.84 % , loss : 3.330601215362549\n",
      "step : 31.28 % , loss : 3.562459707260132\n",
      "step : 31.72 % , loss : 3.5002059936523438\n",
      "step : 32.16 % , loss : 3.5747830867767334\n",
      "step : 32.6 % , loss : 3.3760881423950195\n",
      "step : 33.04 % , loss : 3.4561665058135986\n",
      "step : 33.48 % , loss : 3.397442102432251\n",
      "step : 33.92 % , loss : 3.4873554706573486\n",
      "step : 34.36 % , loss : 3.568479299545288\n",
      "step : 34.8 % , loss : 3.4013612270355225\n",
      "step : 35.24 % , loss : 3.429349899291992\n",
      "step : 35.68 % , loss : 3.42901611328125\n",
      "step : 36.12 % , loss : 3.4756345748901367\n",
      "step : 36.56 % , loss : 3.470365285873413\n",
      "step : 37.0 % , loss : 3.31955885887146\n",
      "step : 37.44 % , loss : 3.3370065689086914\n",
      "step : 37.89 % , loss : 3.3704984188079834\n",
      "step : 38.33 % , loss : 3.560622453689575\n",
      "step : 38.77 % , loss : 3.3387796878814697\n",
      "step : 39.21 % , loss : 3.3780224323272705\n",
      "step : 39.65 % , loss : 3.4583017826080322\n",
      "step : 40.09 % , loss : 3.4257123470306396\n",
      "step : 40.53 % , loss : 3.5201549530029297\n",
      "step : 40.97 % , loss : 3.3835091590881348\n",
      "step : 41.41 % , loss : 3.53218936920166\n",
      "step : 41.85 % , loss : 3.2610113620758057\n",
      "step : 42.29 % , loss : 3.160736083984375\n",
      "step : 42.73 % , loss : 3.430093765258789\n",
      "step : 43.17 % , loss : 3.344484329223633\n",
      "step : 43.61 % , loss : 3.329801082611084\n",
      "step : 44.05 % , loss : 3.3412251472473145\n",
      "step : 44.49 % , loss : 3.313491106033325\n",
      "step : 44.93 % , loss : 3.434122323989868\n",
      "step : 45.37 % , loss : 3.3994574546813965\n",
      "step : 45.81 % , loss : 3.4622607231140137\n",
      "step : 46.26 % , loss : 3.463080883026123\n",
      "step : 46.7 % , loss : 3.2969164848327637\n",
      "step : 47.14 % , loss : 3.3605473041534424\n",
      "step : 47.58 % , loss : 3.2185423374176025\n",
      "step : 48.02 % , loss : 3.3491106033325195\n",
      "step : 48.46 % , loss : 3.5552263259887695\n",
      "step : 48.9 % , loss : 3.2863540649414062\n",
      "step : 49.34 % , loss : 3.4239249229431152\n",
      "step : 49.78 % , loss : 3.402860641479492\n",
      "step : 50.22 % , loss : 3.275094985961914\n",
      "step : 50.66 % , loss : 3.181887149810791\n",
      "step : 51.1 % , loss : 3.383314847946167\n",
      "step : 51.54 % , loss : 3.2579431533813477\n",
      "step : 51.98 % , loss : 3.1838865280151367\n",
      "step : 52.42 % , loss : 3.397369146347046\n",
      "step : 52.86 % , loss : 3.2913429737091064\n",
      "step : 53.3 % , loss : 3.3422811031341553\n",
      "step : 53.74 % , loss : 3.3516428470611572\n",
      "step : 54.19 % , loss : 3.3755099773406982\n",
      "step : 54.63 % , loss : 3.3858680725097656\n",
      "step : 55.07 % , loss : 3.469895124435425\n",
      "step : 55.51 % , loss : 3.2801687717437744\n",
      "step : 55.95 % , loss : 3.3262295722961426\n",
      "step : 56.39 % , loss : 3.424685001373291\n",
      "step : 56.83 % , loss : 3.3639891147613525\n",
      "step : 57.27 % , loss : 3.2831029891967773\n",
      "step : 57.71 % , loss : 3.385161876678467\n",
      "step : 58.15 % , loss : 3.398981809616089\n",
      "step : 58.59 % , loss : 3.271904468536377\n",
      "step : 59.03 % , loss : 3.288975477218628\n",
      "step : 59.47 % , loss : 3.2444136142730713\n",
      "step : 59.91 % , loss : 3.513491630554199\n",
      "step : 60.35 % , loss : 3.528104305267334\n",
      "step : 60.79 % , loss : 3.4202282428741455\n",
      "step : 61.23 % , loss : 3.1595048904418945\n",
      "step : 61.67 % , loss : 3.1697399616241455\n",
      "step : 62.11 % , loss : 3.2978508472442627\n",
      "step : 62.56 % , loss : 3.357471466064453\n",
      "step : 63.0 % , loss : 3.464218854904175\n",
      "step : 63.44 % , loss : 3.1591200828552246\n",
      "step : 63.88 % , loss : 3.2045958042144775\n",
      "step : 64.32 % , loss : 3.2931525707244873\n",
      "step : 64.76 % , loss : 3.367046356201172\n",
      "step : 65.2 % , loss : 3.252725124359131\n",
      "step : 65.64 % , loss : 3.2984108924865723\n",
      "step : 66.08 % , loss : 3.3773956298828125\n",
      "step : 66.52 % , loss : 3.34562349319458\n",
      "step : 66.96 % , loss : 3.408034324645996\n",
      "step : 67.4 % , loss : 3.364497184753418\n",
      "step : 67.84 % , loss : 3.238556146621704\n",
      "step : 68.28 % , loss : 3.354008436203003\n",
      "step : 68.72 % , loss : 3.194523334503174\n",
      "step : 69.16 % , loss : 3.277637004852295\n",
      "step : 69.6 % , loss : 3.28843092918396\n",
      "step : 70.04 % , loss : 3.3374927043914795\n",
      "step : 70.48 % , loss : 3.2377047538757324\n",
      "step : 70.93 % , loss : 3.1737499237060547\n",
      "step : 71.37 % , loss : 3.259838104248047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 71.81 % , loss : 3.3474395275115967\n",
      "step : 72.25 % , loss : 3.3069040775299072\n",
      "step : 72.69 % , loss : 3.2128946781158447\n",
      "step : 73.13 % , loss : 3.3499045372009277\n",
      "step : 73.57 % , loss : 3.3387703895568848\n",
      "step : 74.01 % , loss : 3.1667299270629883\n",
      "step : 74.45 % , loss : 3.3701772689819336\n",
      "step : 74.89 % , loss : 3.124979257583618\n",
      "step : 75.33 % , loss : 3.2104411125183105\n",
      "step : 75.77 % , loss : 3.4442923069000244\n",
      "step : 76.21 % , loss : 3.249596118927002\n",
      "step : 76.65 % , loss : 3.0336382389068604\n",
      "step : 77.09 % , loss : 3.1652135848999023\n",
      "step : 77.53 % , loss : 3.207982301712036\n",
      "step : 77.97 % , loss : 3.3095028400421143\n",
      "step : 78.41 % , loss : 3.341764211654663\n",
      "step : 78.85 % , loss : 3.238327980041504\n",
      "step : 79.3 % , loss : 3.215526819229126\n",
      "step : 79.74 % , loss : 3.115304470062256\n",
      "step : 80.18 % , loss : 3.431718587875366\n",
      "step : 80.62 % , loss : 3.2817583084106445\n",
      "step : 81.06 % , loss : 3.1962320804595947\n",
      "step : 81.5 % , loss : 3.1206374168395996\n",
      "step : 81.94 % , loss : 3.2420263290405273\n",
      "step : 82.38 % , loss : 3.2604520320892334\n",
      "step : 82.82 % , loss : 3.3393051624298096\n",
      "step : 83.26 % , loss : 3.157756805419922\n",
      "step : 83.7 % , loss : 3.177645683288574\n",
      "step : 84.14 % , loss : 3.3931758403778076\n",
      "step : 84.58 % , loss : 3.1289281845092773\n",
      "step : 85.02 % , loss : 3.1609466075897217\n",
      "step : 85.46 % , loss : 3.109600782394409\n",
      "step : 85.9 % , loss : 3.2461977005004883\n",
      "step : 86.34 % , loss : 3.0251946449279785\n",
      "step : 86.78 % , loss : 3.292926073074341\n",
      "step : 87.22 % , loss : 3.1507484912872314\n",
      "step : 87.67 % , loss : 3.161133289337158\n",
      "step : 88.11 % , loss : 2.975186586380005\n",
      "step : 88.55 % , loss : 2.9520249366760254\n",
      "step : 88.99 % , loss : 3.1894991397857666\n",
      "step : 89.43 % , loss : 3.176374673843384\n",
      "step : 89.87 % , loss : 3.084810256958008\n",
      "step : 90.31 % , loss : 3.3259823322296143\n",
      "step : 90.75 % , loss : 3.1035139560699463\n",
      "step : 91.19 % , loss : 3.164987325668335\n",
      "step : 91.63 % , loss : 3.255117416381836\n",
      "step : 92.07 % , loss : 3.089669704437256\n",
      "step : 92.51 % , loss : 3.263195753097534\n",
      "step : 92.95 % , loss : 3.2496659755706787\n",
      "step : 93.39 % , loss : 3.1234946250915527\n",
      "step : 93.83 % , loss : 3.0844171047210693\n",
      "step : 94.27 % , loss : 3.398514747619629\n",
      "step : 94.71 % , loss : 3.0863466262817383\n",
      "step : 95.15 % , loss : 3.2420523166656494\n",
      "step : 95.59 % , loss : 3.137971878051758\n",
      "step : 96.04 % , loss : 3.086246967315674\n",
      "step : 96.48 % , loss : 3.297982931137085\n",
      "step : 96.92 % , loss : 3.012563943862915\n",
      "step : 97.36 % , loss : 3.0496063232421875\n",
      "step : 97.8 % , loss : 3.276456832885742\n",
      "step : 98.24 % , loss : 3.114853858947754\n",
      "step : 98.68 % , loss : 3.0778181552886963\n",
      "step : 99.12 % , loss : 3.2681186199188232\n",
      "step : 99.56 % , loss : 3.0794315338134766\n",
      "Epoch: 02 | Time: 0m 8s\n",
      "\tTrain Loss: 3.376 | Train PPL: 29.256\n",
      "\tValidation Loss: 2.899 | Validation PPL: 18.164\n",
      "step : 0.0 % , loss : 3.0092241764068604\n",
      "step : 0.44 % , loss : 2.912607192993164\n",
      "step : 0.88 % , loss : 3.128004312515259\n",
      "step : 1.32 % , loss : 2.9920294284820557\n",
      "step : 1.76 % , loss : 2.896963357925415\n",
      "step : 2.2 % , loss : 3.068613290786743\n",
      "step : 2.64 % , loss : 3.119903087615967\n",
      "step : 3.08 % , loss : 2.9794578552246094\n",
      "step : 3.52 % , loss : 3.0743374824523926\n",
      "step : 3.96 % , loss : 3.075558662414551\n",
      "step : 4.41 % , loss : 2.8701977729797363\n",
      "step : 4.85 % , loss : 2.940139055252075\n",
      "step : 5.29 % , loss : 2.970250368118286\n",
      "step : 5.73 % , loss : 3.0364906787872314\n",
      "step : 6.17 % , loss : 2.9757931232452393\n",
      "step : 6.61 % , loss : 3.078521966934204\n",
      "step : 7.05 % , loss : 3.0304017066955566\n",
      "step : 7.49 % , loss : 2.8986294269561768\n",
      "step : 7.93 % , loss : 3.0438849925994873\n",
      "step : 8.37 % , loss : 3.1170058250427246\n",
      "step : 8.81 % , loss : 3.1221840381622314\n",
      "step : 9.25 % , loss : 3.0693283081054688\n",
      "step : 9.69 % , loss : 3.1314730644226074\n",
      "step : 10.13 % , loss : 2.861250638961792\n",
      "step : 10.57 % , loss : 3.0481221675872803\n",
      "step : 11.01 % , loss : 3.0290513038635254\n",
      "step : 11.45 % , loss : 2.8623046875\n",
      "step : 11.89 % , loss : 2.9677646160125732\n",
      "step : 12.33 % , loss : 3.091017246246338\n",
      "step : 12.78 % , loss : 2.981868267059326\n",
      "step : 13.22 % , loss : 2.968613862991333\n",
      "step : 13.66 % , loss : 2.9716999530792236\n",
      "step : 14.1 % , loss : 3.0345489978790283\n",
      "step : 14.54 % , loss : 3.0451953411102295\n",
      "step : 14.98 % , loss : 3.0374679565429688\n",
      "step : 15.42 % , loss : 3.0066847801208496\n",
      "step : 15.86 % , loss : 2.9370548725128174\n",
      "step : 16.3 % , loss : 3.0201668739318848\n",
      "step : 16.74 % , loss : 2.9815306663513184\n",
      "step : 17.18 % , loss : 2.958670139312744\n",
      "step : 17.62 % , loss : 2.9293227195739746\n",
      "step : 18.06 % , loss : 3.0006837844848633\n",
      "step : 18.5 % , loss : 3.0405728816986084\n",
      "step : 18.94 % , loss : 3.066277265548706\n",
      "step : 19.38 % , loss : 3.03306245803833\n",
      "step : 19.82 % , loss : 2.902653217315674\n",
      "step : 20.26 % , loss : 3.0262017250061035\n",
      "step : 20.7 % , loss : 3.0790679454803467\n",
      "step : 21.15 % , loss : 3.010133743286133\n",
      "step : 21.59 % , loss : 3.15266752243042\n",
      "step : 22.03 % , loss : 3.2273662090301514\n",
      "step : 22.47 % , loss : 2.981295347213745\n",
      "step : 22.91 % , loss : 2.9661765098571777\n",
      "step : 23.35 % , loss : 2.995626926422119\n",
      "step : 23.79 % , loss : 2.810537576675415\n",
      "step : 24.23 % , loss : 3.0982301235198975\n",
      "step : 24.67 % , loss : 2.9923224449157715\n",
      "step : 25.11 % , loss : 2.9128525257110596\n",
      "step : 25.55 % , loss : 2.88284969329834\n",
      "step : 25.99 % , loss : 3.044586420059204\n",
      "step : 26.43 % , loss : 3.033008575439453\n",
      "step : 26.87 % , loss : 3.0911457538604736\n",
      "step : 27.31 % , loss : 2.8950436115264893\n",
      "step : 27.75 % , loss : 2.9705002307891846\n",
      "step : 28.19 % , loss : 3.0095949172973633\n",
      "step : 28.63 % , loss : 2.8625049591064453\n",
      "step : 29.07 % , loss : 3.0283148288726807\n",
      "step : 29.52 % , loss : 2.948939323425293\n",
      "step : 29.96 % , loss : 2.925995349884033\n",
      "step : 30.4 % , loss : 2.912935972213745\n",
      "step : 30.84 % , loss : 2.9178106784820557\n",
      "step : 31.28 % , loss : 2.964543104171753\n",
      "step : 31.72 % , loss : 2.9373831748962402\n",
      "step : 32.16 % , loss : 2.915989875793457\n",
      "step : 32.6 % , loss : 3.0104494094848633\n",
      "step : 33.04 % , loss : 2.863778591156006\n",
      "step : 33.48 % , loss : 3.050935745239258\n",
      "step : 33.92 % , loss : 2.9774045944213867\n",
      "step : 34.36 % , loss : 2.89371919631958\n",
      "step : 34.8 % , loss : 2.84847354888916\n",
      "step : 35.24 % , loss : 2.917581558227539\n",
      "step : 35.68 % , loss : 2.788459300994873\n",
      "step : 36.12 % , loss : 2.878391981124878\n",
      "step : 36.56 % , loss : 2.9849634170532227\n",
      "step : 37.0 % , loss : 3.004366636276245\n",
      "step : 37.44 % , loss : 2.8972742557525635\n",
      "step : 37.89 % , loss : 2.9318430423736572\n",
      "step : 38.33 % , loss : 3.0317535400390625\n",
      "step : 38.77 % , loss : 2.896622657775879\n",
      "step : 39.21 % , loss : 2.951463222503662\n",
      "step : 39.65 % , loss : 2.9764323234558105\n",
      "step : 40.09 % , loss : 3.007829189300537\n",
      "step : 40.53 % , loss : 2.877354860305786\n",
      "step : 40.97 % , loss : 2.886871337890625\n",
      "step : 41.41 % , loss : 2.83544921875\n",
      "step : 41.85 % , loss : 2.939789295196533\n",
      "step : 42.29 % , loss : 2.9292073249816895\n",
      "step : 42.73 % , loss : 2.9702327251434326\n",
      "step : 43.17 % , loss : 3.013546943664551\n",
      "step : 43.61 % , loss : 2.9093265533447266\n",
      "step : 44.05 % , loss : 2.929755687713623\n",
      "step : 44.49 % , loss : 2.936980724334717\n",
      "step : 44.93 % , loss : 2.8099210262298584\n",
      "step : 45.37 % , loss : 2.971360206604004\n",
      "step : 45.81 % , loss : 2.8109450340270996\n",
      "step : 46.26 % , loss : 2.9464828968048096\n",
      "step : 46.7 % , loss : 2.847588539123535\n",
      "step : 47.14 % , loss : 2.9325356483459473\n",
      "step : 47.58 % , loss : 2.7551891803741455\n",
      "step : 48.02 % , loss : 2.8857905864715576\n",
      "step : 48.46 % , loss : 2.8994534015655518\n",
      "step : 48.9 % , loss : 2.865145444869995\n",
      "step : 49.34 % , loss : 2.9919824600219727\n",
      "step : 49.78 % , loss : 2.9723904132843018\n",
      "step : 50.22 % , loss : 2.9598050117492676\n",
      "step : 50.66 % , loss : 2.9833931922912598\n",
      "step : 51.1 % , loss : 2.7731313705444336\n",
      "step : 51.54 % , loss : 3.0642075538635254\n",
      "step : 51.98 % , loss : 2.9644017219543457\n",
      "step : 52.42 % , loss : 2.7137324810028076\n",
      "step : 52.86 % , loss : 3.010477066040039\n",
      "step : 53.3 % , loss : 2.931589126586914\n",
      "step : 53.74 % , loss : 2.733508825302124\n",
      "step : 54.19 % , loss : 2.7405035495758057\n",
      "step : 54.63 % , loss : 2.9352869987487793\n",
      "step : 55.07 % , loss : 2.83724308013916\n",
      "step : 55.51 % , loss : 2.8972766399383545\n",
      "step : 55.95 % , loss : 2.757997989654541\n",
      "step : 56.39 % , loss : 2.76315975189209\n",
      "step : 56.83 % , loss : 2.835587501525879\n",
      "step : 57.27 % , loss : 2.935051679611206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 57.71 % , loss : 2.8210604190826416\n",
      "step : 58.15 % , loss : 2.8008761405944824\n",
      "step : 58.59 % , loss : 2.9240756034851074\n",
      "step : 59.03 % , loss : 2.695463180541992\n",
      "step : 59.47 % , loss : 2.8344340324401855\n",
      "step : 59.91 % , loss : 3.1152236461639404\n",
      "step : 60.35 % , loss : 2.8179244995117188\n",
      "step : 60.79 % , loss : 2.803516387939453\n",
      "step : 61.23 % , loss : 2.8730926513671875\n",
      "step : 61.67 % , loss : 3.0276968479156494\n",
      "step : 62.11 % , loss : 2.7417185306549072\n",
      "step : 62.56 % , loss : 2.740041732788086\n",
      "step : 63.0 % , loss : 2.899197578430176\n",
      "step : 63.44 % , loss : 2.743548631668091\n",
      "step : 63.88 % , loss : 2.8847649097442627\n",
      "step : 64.32 % , loss : 2.9106006622314453\n",
      "step : 64.76 % , loss : 2.8033502101898193\n",
      "step : 65.2 % , loss : 2.8529601097106934\n",
      "step : 65.64 % , loss : 2.9090232849121094\n",
      "step : 66.08 % , loss : 2.849066734313965\n",
      "step : 66.52 % , loss : 2.891796827316284\n",
      "step : 66.96 % , loss : 2.649324893951416\n",
      "step : 67.4 % , loss : 2.8628969192504883\n",
      "step : 67.84 % , loss : 2.7761147022247314\n",
      "step : 68.28 % , loss : 2.887942314147949\n",
      "step : 68.72 % , loss : 2.9728474617004395\n",
      "step : 69.16 % , loss : 2.8994839191436768\n",
      "step : 69.6 % , loss : 2.6061556339263916\n",
      "step : 70.04 % , loss : 2.767484664916992\n",
      "step : 70.48 % , loss : 2.9034247398376465\n",
      "step : 70.93 % , loss : 2.849097490310669\n",
      "step : 71.37 % , loss : 2.713392496109009\n",
      "step : 71.81 % , loss : 2.977365016937256\n",
      "step : 72.25 % , loss : 2.9302499294281006\n",
      "step : 72.69 % , loss : 2.859344244003296\n",
      "step : 73.13 % , loss : 2.705779790878296\n",
      "step : 73.57 % , loss : 2.9815096855163574\n",
      "step : 74.01 % , loss : 2.9205007553100586\n",
      "step : 74.45 % , loss : 2.7868175506591797\n",
      "step : 74.89 % , loss : 2.9120681285858154\n",
      "step : 75.33 % , loss : 2.8834280967712402\n",
      "step : 75.77 % , loss : 2.8058767318725586\n",
      "step : 76.21 % , loss : 2.9547030925750732\n",
      "step : 76.65 % , loss : 2.6691598892211914\n",
      "step : 77.09 % , loss : 2.8053178787231445\n",
      "step : 77.53 % , loss : 2.824761152267456\n",
      "step : 77.97 % , loss : 2.798588991165161\n",
      "step : 78.41 % , loss : 2.865933418273926\n",
      "step : 78.85 % , loss : 3.0189523696899414\n",
      "step : 79.3 % , loss : 3.067415952682495\n",
      "step : 79.74 % , loss : 2.7785146236419678\n",
      "step : 80.18 % , loss : 2.773672103881836\n",
      "step : 80.62 % , loss : 2.8439197540283203\n",
      "step : 81.06 % , loss : 2.823925495147705\n",
      "step : 81.5 % , loss : 2.6508800983428955\n",
      "step : 81.94 % , loss : 2.6490983963012695\n",
      "step : 82.38 % , loss : 2.7758281230926514\n",
      "step : 82.82 % , loss : 2.7858855724334717\n",
      "step : 83.26 % , loss : 2.867217779159546\n",
      "step : 83.7 % , loss : 2.7838449478149414\n",
      "step : 84.14 % , loss : 2.908946990966797\n",
      "step : 84.58 % , loss : 2.7723331451416016\n",
      "step : 85.02 % , loss : 2.7676775455474854\n",
      "step : 85.46 % , loss : 2.860365867614746\n",
      "step : 85.9 % , loss : 2.690045118331909\n",
      "step : 86.34 % , loss : 2.948847770690918\n",
      "step : 86.78 % , loss : 2.7578582763671875\n",
      "step : 87.22 % , loss : 2.738961935043335\n",
      "step : 87.67 % , loss : 2.8187880516052246\n",
      "step : 88.11 % , loss : 2.814504384994507\n",
      "step : 88.55 % , loss : 2.9508092403411865\n",
      "step : 88.99 % , loss : 2.8045594692230225\n",
      "step : 89.43 % , loss : 2.680109977722168\n",
      "step : 89.87 % , loss : 2.7791802883148193\n",
      "step : 90.31 % , loss : 2.7102510929107666\n",
      "step : 90.75 % , loss : 2.7108914852142334\n",
      "step : 91.19 % , loss : 2.8714141845703125\n",
      "step : 91.63 % , loss : 2.8304574489593506\n",
      "step : 92.07 % , loss : 2.7718100547790527\n",
      "step : 92.51 % , loss : 2.764741897583008\n",
      "step : 92.95 % , loss : 2.7274813652038574\n",
      "step : 93.39 % , loss : 2.877389430999756\n",
      "step : 93.83 % , loss : 2.7974889278411865\n",
      "step : 94.27 % , loss : 2.7571980953216553\n",
      "step : 94.71 % , loss : 2.9179036617279053\n",
      "step : 95.15 % , loss : 2.785207748413086\n",
      "step : 95.59 % , loss : 2.7946505546569824\n",
      "step : 96.04 % , loss : 2.6900041103363037\n",
      "step : 96.48 % , loss : 2.7923808097839355\n",
      "step : 96.92 % , loss : 2.666344165802002\n",
      "step : 97.36 % , loss : 2.858660936355591\n",
      "step : 97.8 % , loss : 2.5129566192626953\n",
      "step : 98.24 % , loss : 2.784407377243042\n",
      "step : 98.68 % , loss : 2.644763946533203\n",
      "step : 99.12 % , loss : 2.6878182888031006\n",
      "step : 99.56 % , loss : 2.875946521759033\n",
      "Epoch: 03 | Time: 0m 8s\n",
      "\tTrain Loss: 2.899 | Train PPL: 18.152\n",
      "\tValidation Loss: 2.540 | Validation PPL: 12.677\n",
      "step : 0.0 % , loss : 2.5739831924438477\n",
      "step : 0.44 % , loss : 2.669238567352295\n",
      "step : 0.88 % , loss : 2.750980854034424\n",
      "step : 1.32 % , loss : 2.6101441383361816\n",
      "step : 1.76 % , loss : 2.71356463432312\n",
      "step : 2.2 % , loss : 2.672274112701416\n",
      "step : 2.64 % , loss : 2.6724812984466553\n",
      "step : 3.08 % , loss : 2.6544992923736572\n",
      "step : 3.52 % , loss : 2.6924686431884766\n",
      "step : 3.96 % , loss : 2.6417150497436523\n",
      "step : 4.41 % , loss : 2.436105251312256\n",
      "step : 4.85 % , loss : 2.650129556655884\n",
      "step : 5.29 % , loss : 2.6188201904296875\n",
      "step : 5.73 % , loss : 2.4734671115875244\n",
      "step : 6.17 % , loss : 2.651475191116333\n",
      "step : 6.61 % , loss : 2.549984931945801\n",
      "step : 7.05 % , loss : 2.602534055709839\n",
      "step : 7.49 % , loss : 2.5606653690338135\n",
      "step : 7.93 % , loss : 2.48435115814209\n",
      "step : 8.37 % , loss : 2.554575204849243\n",
      "step : 8.81 % , loss : 2.675309419631958\n",
      "step : 9.25 % , loss : 2.5444772243499756\n",
      "step : 9.69 % , loss : 2.5870134830474854\n",
      "step : 10.13 % , loss : 2.6423566341400146\n",
      "step : 10.57 % , loss : 2.6520869731903076\n",
      "step : 11.01 % , loss : 2.555349826812744\n",
      "step : 11.45 % , loss : 2.6742289066314697\n",
      "step : 11.89 % , loss : 2.6094987392425537\n",
      "step : 12.33 % , loss : 2.5269901752471924\n",
      "step : 12.78 % , loss : 2.6743311882019043\n",
      "step : 13.22 % , loss : 2.558889627456665\n",
      "step : 13.66 % , loss : 2.5481841564178467\n",
      "step : 14.1 % , loss : 2.7429537773132324\n",
      "step : 14.54 % , loss : 2.602773427963257\n",
      "step : 14.98 % , loss : 2.548938751220703\n",
      "step : 15.42 % , loss : 2.678574323654175\n",
      "step : 15.86 % , loss : 2.7221596240997314\n",
      "step : 16.3 % , loss : 2.536255121231079\n",
      "step : 16.74 % , loss : 2.7432823181152344\n",
      "step : 17.18 % , loss : 2.6928186416625977\n",
      "step : 17.62 % , loss : 2.6830899715423584\n",
      "step : 18.06 % , loss : 2.6947970390319824\n",
      "step : 18.5 % , loss : 2.583955764770508\n",
      "step : 18.94 % , loss : 2.646347999572754\n",
      "step : 19.38 % , loss : 2.7790515422821045\n",
      "step : 19.82 % , loss : 2.5760819911956787\n",
      "step : 20.26 % , loss : 2.688450813293457\n",
      "step : 20.7 % , loss : 2.656400203704834\n",
      "step : 21.15 % , loss : 2.5913338661193848\n",
      "step : 21.59 % , loss : 2.558839797973633\n",
      "step : 22.03 % , loss : 2.6026206016540527\n",
      "step : 22.47 % , loss : 2.5339195728302\n",
      "step : 22.91 % , loss : 2.6369786262512207\n",
      "step : 23.35 % , loss : 2.7780542373657227\n",
      "step : 23.79 % , loss : 2.629685878753662\n",
      "step : 24.23 % , loss : 2.729449510574341\n",
      "step : 24.67 % , loss : 2.4179491996765137\n",
      "step : 25.11 % , loss : 2.4450786113739014\n",
      "step : 25.55 % , loss : 2.6053647994995117\n",
      "step : 25.99 % , loss : 2.678901433944702\n",
      "step : 26.43 % , loss : 2.730297803878784\n",
      "step : 26.87 % , loss : 2.610722541809082\n",
      "step : 27.31 % , loss : 2.6563010215759277\n",
      "step : 27.75 % , loss : 2.619884729385376\n",
      "step : 28.19 % , loss : 2.5503945350646973\n",
      "step : 28.63 % , loss : 2.6328823566436768\n",
      "step : 29.07 % , loss : 2.5764834880828857\n",
      "step : 29.52 % , loss : 2.547841787338257\n",
      "step : 29.96 % , loss : 2.6356849670410156\n",
      "step : 30.4 % , loss : 2.557366132736206\n",
      "step : 30.84 % , loss : 2.5897912979125977\n",
      "step : 31.28 % , loss : 2.6915924549102783\n",
      "step : 31.72 % , loss : 2.6962316036224365\n",
      "step : 32.16 % , loss : 2.5509352684020996\n",
      "step : 32.6 % , loss : 2.613145112991333\n",
      "step : 33.04 % , loss : 2.588088035583496\n",
      "step : 33.48 % , loss : 2.4274306297302246\n",
      "step : 33.92 % , loss : 2.514005661010742\n",
      "step : 34.36 % , loss : 2.468470573425293\n",
      "step : 34.8 % , loss : 2.650535821914673\n",
      "step : 35.24 % , loss : 2.53761887550354\n",
      "step : 35.68 % , loss : 2.6431806087493896\n",
      "step : 36.12 % , loss : 2.5727314949035645\n",
      "step : 36.56 % , loss : 2.674832820892334\n",
      "step : 37.0 % , loss : 2.4908013343811035\n",
      "step : 37.44 % , loss : 2.8483424186706543\n",
      "step : 37.89 % , loss : 2.668748378753662\n",
      "step : 38.33 % , loss : 2.599503517150879\n",
      "step : 38.77 % , loss : 2.5445315837860107\n",
      "step : 39.21 % , loss : 2.6983048915863037\n",
      "step : 39.65 % , loss : 2.5533738136291504\n",
      "step : 40.09 % , loss : 2.5691747665405273\n",
      "step : 40.53 % , loss : 2.6087446212768555\n",
      "step : 40.97 % , loss : 2.5297255516052246\n",
      "step : 41.41 % , loss : 2.428936243057251\n",
      "step : 41.85 % , loss : 2.8515350818634033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 42.29 % , loss : 2.675055742263794\n",
      "step : 42.73 % , loss : 2.6089251041412354\n",
      "step : 43.17 % , loss : 2.558142900466919\n",
      "step : 43.61 % , loss : 2.5719571113586426\n",
      "step : 44.05 % , loss : 2.5877208709716797\n",
      "step : 44.49 % , loss : 2.5588619709014893\n",
      "step : 44.93 % , loss : 2.479030132293701\n",
      "step : 45.37 % , loss : 2.666048765182495\n",
      "step : 45.81 % , loss : 2.5640385150909424\n",
      "step : 46.26 % , loss : 2.430650472640991\n",
      "step : 46.7 % , loss : 2.439948320388794\n",
      "step : 47.14 % , loss : 2.4175829887390137\n",
      "step : 47.58 % , loss : 2.611564874649048\n",
      "step : 48.02 % , loss : 2.5779378414154053\n",
      "step : 48.46 % , loss : 2.4727261066436768\n",
      "step : 48.9 % , loss : 2.565427780151367\n",
      "step : 49.34 % , loss : 2.5440425872802734\n",
      "step : 49.78 % , loss : 2.6068530082702637\n",
      "step : 50.22 % , loss : 2.5398619174957275\n",
      "step : 50.66 % , loss : 2.6005334854125977\n",
      "step : 51.1 % , loss : 2.635530471801758\n",
      "step : 51.54 % , loss : 2.342151403427124\n",
      "step : 51.98 % , loss : 2.572678804397583\n",
      "step : 52.42 % , loss : 2.4255459308624268\n",
      "step : 52.86 % , loss : 2.7769529819488525\n",
      "step : 53.3 % , loss : 2.586775064468384\n",
      "step : 53.74 % , loss : 2.6508452892303467\n",
      "step : 54.19 % , loss : 2.4860599040985107\n",
      "step : 54.63 % , loss : 2.596937656402588\n",
      "step : 55.07 % , loss : 2.510460376739502\n",
      "step : 55.51 % , loss : 2.5786004066467285\n",
      "step : 55.95 % , loss : 2.621431589126587\n",
      "step : 56.39 % , loss : 2.399679183959961\n",
      "step : 56.83 % , loss : 2.547863721847534\n",
      "step : 57.27 % , loss : 2.6011931896209717\n",
      "step : 57.71 % , loss : 2.5515849590301514\n",
      "step : 58.15 % , loss : 2.556610107421875\n",
      "step : 58.59 % , loss : 2.6851391792297363\n",
      "step : 59.03 % , loss : 2.6508610248565674\n",
      "step : 59.47 % , loss : 2.517498254776001\n",
      "step : 59.91 % , loss : 2.5314934253692627\n",
      "step : 60.35 % , loss : 2.52020525932312\n",
      "step : 60.79 % , loss : 2.7591280937194824\n",
      "step : 61.23 % , loss : 2.6378533840179443\n",
      "step : 61.67 % , loss : 2.7441134452819824\n",
      "step : 62.11 % , loss : 2.649933338165283\n",
      "step : 62.56 % , loss : 2.4069712162017822\n",
      "step : 63.0 % , loss : 2.569486618041992\n",
      "step : 63.44 % , loss : 2.4685142040252686\n",
      "step : 63.88 % , loss : 2.491227626800537\n",
      "step : 64.32 % , loss : 2.4518203735351562\n",
      "step : 64.76 % , loss : 2.5879967212677\n",
      "step : 65.2 % , loss : 2.520542621612549\n",
      "step : 65.64 % , loss : 2.550107002258301\n",
      "step : 66.08 % , loss : 2.5886573791503906\n",
      "step : 66.52 % , loss : 2.53371262550354\n",
      "step : 66.96 % , loss : 2.5606865882873535\n",
      "step : 67.4 % , loss : 2.513087511062622\n",
      "step : 67.84 % , loss : 2.4481208324432373\n",
      "step : 68.28 % , loss : 2.59855055809021\n",
      "step : 68.72 % , loss : 2.6142899990081787\n",
      "step : 69.16 % , loss : 2.4414772987365723\n",
      "step : 69.6 % , loss : 2.7323262691497803\n",
      "step : 70.04 % , loss : 2.5364797115325928\n",
      "step : 70.48 % , loss : 2.475670099258423\n",
      "step : 70.93 % , loss : 2.5749967098236084\n",
      "step : 71.37 % , loss : 2.634955406188965\n",
      "step : 71.81 % , loss : 2.64591121673584\n",
      "step : 72.25 % , loss : 2.4803383350372314\n",
      "step : 72.69 % , loss : 2.481715202331543\n",
      "step : 73.13 % , loss : 2.612396717071533\n",
      "step : 73.57 % , loss : 2.4989397525787354\n",
      "step : 74.01 % , loss : 2.5786571502685547\n",
      "step : 74.45 % , loss : 2.4140260219573975\n",
      "step : 74.89 % , loss : 2.396299123764038\n",
      "step : 75.33 % , loss : 2.4881436824798584\n",
      "step : 75.77 % , loss : 2.365225076675415\n",
      "step : 76.21 % , loss : 2.3517227172851562\n",
      "step : 76.65 % , loss : 2.405923366546631\n",
      "step : 77.09 % , loss : 2.5690677165985107\n",
      "step : 77.53 % , loss : 2.575056791305542\n",
      "step : 77.97 % , loss : 2.4907636642456055\n",
      "step : 78.41 % , loss : 2.6895105838775635\n",
      "step : 78.85 % , loss : 2.4559314250946045\n",
      "step : 79.3 % , loss : 2.5121428966522217\n",
      "step : 79.74 % , loss : 2.587984800338745\n",
      "step : 80.18 % , loss : 2.501763105392456\n",
      "step : 80.62 % , loss : 2.5785837173461914\n",
      "step : 81.06 % , loss : 2.3182528018951416\n",
      "step : 81.5 % , loss : 2.7561471462249756\n",
      "step : 81.94 % , loss : 2.426997423171997\n",
      "step : 82.38 % , loss : 2.403682231903076\n",
      "step : 82.82 % , loss : 2.4474053382873535\n",
      "step : 83.26 % , loss : 2.3835535049438477\n",
      "step : 83.7 % , loss : 2.549560785293579\n",
      "step : 84.14 % , loss : 2.4471023082733154\n",
      "step : 84.58 % , loss : 2.3918673992156982\n",
      "step : 85.02 % , loss : 2.5434772968292236\n",
      "step : 85.46 % , loss : 2.4015302658081055\n",
      "step : 85.9 % , loss : 2.6357815265655518\n",
      "step : 86.34 % , loss : 2.511002779006958\n",
      "step : 86.78 % , loss : 2.577709436416626\n",
      "step : 87.22 % , loss : 2.669769048690796\n",
      "step : 87.67 % , loss : 2.502183675765991\n",
      "step : 88.11 % , loss : 2.5938825607299805\n",
      "step : 88.55 % , loss : 2.525879144668579\n",
      "step : 88.99 % , loss : 2.495913028717041\n",
      "step : 89.43 % , loss : 2.5219812393188477\n",
      "step : 89.87 % , loss : 2.569784164428711\n",
      "step : 90.31 % , loss : 2.48417067527771\n",
      "step : 90.75 % , loss : 2.4656600952148438\n",
      "step : 91.19 % , loss : 2.4163265228271484\n",
      "step : 91.63 % , loss : 2.366926670074463\n",
      "step : 92.07 % , loss : 2.521510124206543\n",
      "step : 92.51 % , loss : 2.3599965572357178\n",
      "step : 92.95 % , loss : 2.432856321334839\n",
      "step : 93.39 % , loss : 2.315215826034546\n",
      "step : 93.83 % , loss : 2.4611623287200928\n",
      "step : 94.27 % , loss : 2.3882813453674316\n",
      "step : 94.71 % , loss : 2.497519016265869\n",
      "step : 95.15 % , loss : 2.518308401107788\n",
      "step : 95.59 % , loss : 2.322690725326538\n",
      "step : 96.04 % , loss : 2.5186281204223633\n",
      "step : 96.48 % , loss : 2.495763063430786\n",
      "step : 96.92 % , loss : 2.374149799346924\n",
      "step : 97.36 % , loss : 2.5884695053100586\n",
      "step : 97.8 % , loss : 2.5328078269958496\n",
      "step : 98.24 % , loss : 2.3239471912384033\n",
      "step : 98.68 % , loss : 2.3999922275543213\n",
      "step : 99.12 % , loss : 2.5346460342407227\n",
      "step : 99.56 % , loss : 2.574631929397583\n",
      "Epoch: 04 | Time: 0m 8s\n",
      "\tTrain Loss: 2.563 | Train PPL: 12.972\n",
      "\tValidation Loss: 2.281 | Validation PPL: 9.789\n",
      "step : 0.0 % , loss : 2.3791749477386475\n",
      "step : 0.44 % , loss : 2.234300136566162\n",
      "step : 0.88 % , loss : 2.304614543914795\n",
      "step : 1.32 % , loss : 2.2505321502685547\n",
      "step : 1.76 % , loss : 2.226585865020752\n",
      "step : 2.2 % , loss : 2.380859136581421\n",
      "step : 2.64 % , loss : 2.217273235321045\n",
      "step : 3.08 % , loss : 2.3119757175445557\n",
      "step : 3.52 % , loss : 2.3665380477905273\n",
      "step : 3.96 % , loss : 2.310530424118042\n",
      "step : 4.41 % , loss : 2.3212661743164062\n",
      "step : 4.85 % , loss : 2.4231009483337402\n",
      "step : 5.29 % , loss : 2.3438825607299805\n",
      "step : 5.73 % , loss : 2.467351198196411\n",
      "step : 6.17 % , loss : 2.292236089706421\n",
      "step : 6.61 % , loss : 2.31296443939209\n",
      "step : 7.05 % , loss : 2.3912036418914795\n",
      "step : 7.49 % , loss : 2.4713611602783203\n",
      "step : 7.93 % , loss : 2.540645122528076\n",
      "step : 8.37 % , loss : 2.429114818572998\n",
      "step : 8.81 % , loss : 2.2006208896636963\n",
      "step : 9.25 % , loss : 2.4025018215179443\n",
      "step : 9.69 % , loss : 2.3095810413360596\n",
      "step : 10.13 % , loss : 2.250972270965576\n",
      "step : 10.57 % , loss : 2.40022873878479\n",
      "step : 11.01 % , loss : 2.381974220275879\n",
      "step : 11.45 % , loss : 2.3384153842926025\n",
      "step : 11.89 % , loss : 2.3067898750305176\n",
      "step : 12.33 % , loss : 2.2694308757781982\n",
      "step : 12.78 % , loss : 2.414421319961548\n",
      "step : 13.22 % , loss : 2.4435787200927734\n",
      "step : 13.66 % , loss : 2.148756980895996\n",
      "step : 14.1 % , loss : 2.387213706970215\n",
      "step : 14.54 % , loss : 2.257800340652466\n",
      "step : 14.98 % , loss : 2.269324779510498\n",
      "step : 15.42 % , loss : 2.275068998336792\n",
      "step : 15.86 % , loss : 2.4355430603027344\n",
      "step : 16.3 % , loss : 2.465512752532959\n",
      "step : 16.74 % , loss : 2.34431529045105\n",
      "step : 17.18 % , loss : 2.209099531173706\n",
      "step : 17.62 % , loss : 2.25897216796875\n",
      "step : 18.06 % , loss : 2.4724597930908203\n",
      "step : 18.5 % , loss : 2.2674787044525146\n",
      "step : 18.94 % , loss : 2.331718921661377\n",
      "step : 19.38 % , loss : 2.2518749237060547\n",
      "step : 19.82 % , loss : 2.3044495582580566\n",
      "step : 20.26 % , loss : 2.305859088897705\n",
      "step : 20.7 % , loss : 2.4589507579803467\n",
      "step : 21.15 % , loss : 2.4270989894866943\n",
      "step : 21.59 % , loss : 2.241750717163086\n",
      "step : 22.03 % , loss : 2.4503021240234375\n",
      "step : 22.47 % , loss : 2.465397596359253\n",
      "step : 22.91 % , loss : 2.4844298362731934\n",
      "step : 23.35 % , loss : 2.3501038551330566\n",
      "step : 23.79 % , loss : 2.2576792240142822\n",
      "step : 24.23 % , loss : 2.3376412391662598\n",
      "step : 24.67 % , loss : 2.2370553016662598\n",
      "step : 25.11 % , loss : 2.2558889389038086\n",
      "step : 25.55 % , loss : 2.370096206665039\n",
      "step : 25.99 % , loss : 2.215841293334961\n",
      "step : 26.43 % , loss : 2.3656370639801025\n",
      "step : 26.87 % , loss : 2.406590223312378\n",
      "step : 27.31 % , loss : 2.420060634613037\n",
      "step : 27.75 % , loss : 2.35189151763916\n",
      "step : 28.19 % , loss : 2.299349784851074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 28.63 % , loss : 2.410935163497925\n",
      "step : 29.07 % , loss : 2.426849126815796\n",
      "step : 29.52 % , loss : 2.3442254066467285\n",
      "step : 29.96 % , loss : 2.2287514209747314\n",
      "step : 30.4 % , loss : 2.120941400527954\n",
      "step : 30.84 % , loss : 2.308159112930298\n",
      "step : 31.28 % , loss : 2.3409712314605713\n",
      "step : 31.72 % , loss : 2.392653703689575\n",
      "step : 32.16 % , loss : 2.37748122215271\n",
      "step : 32.6 % , loss : 2.199157953262329\n",
      "step : 33.04 % , loss : 2.198206901550293\n",
      "step : 33.48 % , loss : 2.362462282180786\n",
      "step : 33.92 % , loss : 2.3515005111694336\n",
      "step : 34.36 % , loss : 2.257678747177124\n",
      "step : 34.8 % , loss : 2.1659750938415527\n",
      "step : 35.24 % , loss : 2.3233070373535156\n",
      "step : 35.68 % , loss : 2.282806873321533\n",
      "step : 36.12 % , loss : 2.424041509628296\n",
      "step : 36.56 % , loss : 2.2796854972839355\n",
      "step : 37.0 % , loss : 2.185537099838257\n",
      "step : 37.44 % , loss : 2.3973941802978516\n",
      "step : 37.89 % , loss : 2.2983126640319824\n",
      "step : 38.33 % , loss : 2.390939235687256\n",
      "step : 38.77 % , loss : 2.3097434043884277\n",
      "step : 39.21 % , loss : 2.4818527698516846\n",
      "step : 39.65 % , loss : 2.5057523250579834\n",
      "step : 40.09 % , loss : 2.3644320964813232\n",
      "step : 40.53 % , loss : 2.402170181274414\n",
      "step : 40.97 % , loss : 2.364654779434204\n",
      "step : 41.41 % , loss : 2.231977701187134\n",
      "step : 41.85 % , loss : 2.3636553287506104\n",
      "step : 42.29 % , loss : 2.310236692428589\n",
      "step : 42.73 % , loss : 2.4676458835601807\n",
      "step : 43.17 % , loss : 2.372220277786255\n",
      "step : 43.61 % , loss : 2.325735092163086\n",
      "step : 44.05 % , loss : 2.2842185497283936\n",
      "step : 44.49 % , loss : 2.3443527221679688\n",
      "step : 44.93 % , loss : 2.3183774948120117\n",
      "step : 45.37 % , loss : 2.236922264099121\n",
      "step : 45.81 % , loss : 2.297327756881714\n",
      "step : 46.26 % , loss : 2.0886778831481934\n",
      "step : 46.7 % , loss : 2.1697838306427\n",
      "step : 47.14 % , loss : 2.33506178855896\n",
      "step : 47.58 % , loss : 2.3548762798309326\n",
      "step : 48.02 % , loss : 2.353893518447876\n",
      "step : 48.46 % , loss : 2.3590104579925537\n",
      "step : 48.9 % , loss : 2.2536988258361816\n",
      "step : 49.34 % , loss : 2.3917038440704346\n",
      "step : 49.78 % , loss : 2.1842243671417236\n",
      "step : 50.22 % , loss : 2.293166160583496\n",
      "step : 50.66 % , loss : 2.4774246215820312\n",
      "step : 51.1 % , loss : 2.368241786956787\n",
      "step : 51.54 % , loss : 2.248080015182495\n",
      "step : 51.98 % , loss : 2.30562686920166\n",
      "step : 52.42 % , loss : 2.3466784954071045\n",
      "step : 52.86 % , loss : 2.3947317600250244\n",
      "step : 53.3 % , loss : 2.2140700817108154\n",
      "step : 53.74 % , loss : 2.3354859352111816\n",
      "step : 54.19 % , loss : 2.4296352863311768\n",
      "step : 54.63 % , loss : 2.2170724868774414\n",
      "step : 55.07 % , loss : 2.2581114768981934\n",
      "step : 55.51 % , loss : 2.358200788497925\n",
      "step : 55.95 % , loss : 2.322037935256958\n",
      "step : 56.39 % , loss : 2.317260503768921\n",
      "step : 56.83 % , loss : 2.401444673538208\n",
      "step : 57.27 % , loss : 2.3676939010620117\n",
      "step : 57.71 % , loss : 2.2223246097564697\n",
      "step : 58.15 % , loss : 2.2971222400665283\n",
      "step : 58.59 % , loss : 2.184692144393921\n",
      "step : 59.03 % , loss : 2.1332786083221436\n",
      "step : 59.47 % , loss : 2.2859995365142822\n",
      "step : 59.91 % , loss : 2.4003355503082275\n",
      "step : 60.35 % , loss : 2.2398576736450195\n",
      "step : 60.79 % , loss : 2.127095937728882\n",
      "step : 61.23 % , loss : 2.264540910720825\n",
      "step : 61.67 % , loss : 2.443970203399658\n",
      "step : 62.11 % , loss : 2.33597993850708\n",
      "step : 62.56 % , loss : 2.3056914806365967\n",
      "step : 63.0 % , loss : 2.2550671100616455\n",
      "step : 63.44 % , loss : 2.386370897293091\n",
      "step : 63.88 % , loss : 2.2466375827789307\n",
      "step : 64.32 % , loss : 2.358922243118286\n",
      "step : 64.76 % , loss : 2.304823875427246\n",
      "step : 65.2 % , loss : 2.304769277572632\n",
      "step : 65.64 % , loss : 2.3203797340393066\n",
      "step : 66.08 % , loss : 2.2907159328460693\n",
      "step : 66.52 % , loss : 2.1506056785583496\n",
      "step : 66.96 % , loss : 2.2318925857543945\n",
      "step : 67.4 % , loss : 2.288870334625244\n",
      "step : 67.84 % , loss : 2.3058242797851562\n",
      "step : 68.28 % , loss : 2.273297071456909\n",
      "step : 68.72 % , loss : 2.2412338256835938\n",
      "step : 69.16 % , loss : 2.415022373199463\n",
      "step : 69.6 % , loss : 2.356448173522949\n",
      "step : 70.04 % , loss : 2.235264301300049\n",
      "step : 70.48 % , loss : 2.238250732421875\n",
      "step : 70.93 % , loss : 2.319427967071533\n",
      "step : 71.37 % , loss : 2.3053340911865234\n",
      "step : 71.81 % , loss : 2.341571569442749\n",
      "step : 72.25 % , loss : 2.337920665740967\n",
      "step : 72.69 % , loss : 2.39583420753479\n",
      "step : 73.13 % , loss : 2.24043607711792\n",
      "step : 73.57 % , loss : 2.18510103225708\n",
      "step : 74.01 % , loss : 2.346924304962158\n",
      "step : 74.45 % , loss : 2.23714280128479\n",
      "step : 74.89 % , loss : 2.213681936264038\n",
      "step : 75.33 % , loss : 2.2047014236450195\n",
      "step : 75.77 % , loss : 2.435187339782715\n",
      "step : 76.21 % , loss : 2.2238283157348633\n",
      "step : 76.65 % , loss : 2.2303857803344727\n",
      "step : 77.09 % , loss : 2.276521682739258\n",
      "step : 77.53 % , loss : 2.2798240184783936\n",
      "step : 77.97 % , loss : 2.191957950592041\n",
      "step : 78.41 % , loss : 2.139723062515259\n",
      "step : 78.85 % , loss : 2.340352773666382\n",
      "step : 79.3 % , loss : 2.3447632789611816\n",
      "step : 79.74 % , loss : 2.0977396965026855\n",
      "step : 80.18 % , loss : 2.2414145469665527\n",
      "step : 80.62 % , loss : 2.3559532165527344\n",
      "step : 81.06 % , loss : 2.1837573051452637\n",
      "step : 81.5 % , loss : 2.456239938735962\n",
      "step : 81.94 % , loss : 2.2146401405334473\n",
      "step : 82.38 % , loss : 2.3752787113189697\n",
      "step : 82.82 % , loss : 2.2947821617126465\n",
      "step : 83.26 % , loss : 2.165269136428833\n",
      "step : 83.7 % , loss : 2.320988655090332\n",
      "step : 84.14 % , loss : 2.19075345993042\n",
      "step : 84.58 % , loss : 2.322754383087158\n",
      "step : 85.02 % , loss : 2.257009983062744\n",
      "step : 85.46 % , loss : 2.183940887451172\n",
      "step : 85.9 % , loss : 2.195625066757202\n",
      "step : 86.34 % , loss : 2.2994754314422607\n",
      "step : 86.78 % , loss : 2.394623041152954\n",
      "step : 87.22 % , loss : 2.308116912841797\n",
      "step : 87.67 % , loss : 2.1617014408111572\n",
      "step : 88.11 % , loss : 2.4168801307678223\n",
      "step : 88.55 % , loss : 2.3725638389587402\n",
      "step : 88.99 % , loss : 2.302391290664673\n",
      "step : 89.43 % , loss : 2.345655679702759\n",
      "step : 89.87 % , loss : 2.1925442218780518\n",
      "step : 90.31 % , loss : 2.280193567276001\n",
      "step : 90.75 % , loss : 2.2358219623565674\n",
      "step : 91.19 % , loss : 2.325655460357666\n",
      "step : 91.63 % , loss : 2.3225276470184326\n",
      "step : 92.07 % , loss : 2.3317394256591797\n",
      "step : 92.51 % , loss : 2.3340282440185547\n",
      "step : 92.95 % , loss : 2.455669403076172\n",
      "step : 93.39 % , loss : 2.2129149436950684\n",
      "step : 93.83 % , loss : 2.3786098957061768\n",
      "step : 94.27 % , loss : 2.152238607406616\n",
      "step : 94.71 % , loss : 2.267498016357422\n",
      "step : 95.15 % , loss : 2.235930919647217\n",
      "step : 95.59 % , loss : 2.281129837036133\n",
      "step : 96.04 % , loss : 2.180830955505371\n",
      "step : 96.48 % , loss : 2.2633168697357178\n",
      "step : 96.92 % , loss : 2.0989179611206055\n",
      "step : 97.36 % , loss : 2.433131456375122\n",
      "step : 97.8 % , loss : 2.3364248275756836\n",
      "step : 98.24 % , loss : 2.073716640472412\n",
      "step : 98.68 % , loss : 2.2793068885803223\n",
      "step : 99.12 % , loss : 2.261242151260376\n",
      "step : 99.56 % , loss : 2.306964874267578\n",
      "Epoch: 05 | Time: 0m 8s\n",
      "\tTrain Loss: 2.308 | Train PPL: 10.058\n",
      "\tValidation Loss: 2.113 | Validation PPL: 8.271\n",
      "step : 0.0 % , loss : 2.3348946571350098\n",
      "step : 0.44 % , loss : 2.02255916595459\n",
      "step : 0.88 % , loss : 2.0544633865356445\n",
      "step : 1.32 % , loss : 2.09454345703125\n",
      "step : 1.76 % , loss : 2.093644142150879\n",
      "step : 2.2 % , loss : 2.1824350357055664\n",
      "step : 2.64 % , loss : 2.1045100688934326\n",
      "step : 3.08 % , loss : 2.3355138301849365\n",
      "step : 3.52 % , loss : 2.1016790866851807\n",
      "step : 3.96 % , loss : 2.2076361179351807\n",
      "step : 4.41 % , loss : 2.0063066482543945\n",
      "step : 4.85 % , loss : 2.0576629638671875\n",
      "step : 5.29 % , loss : 2.191237449645996\n",
      "step : 5.73 % , loss : 2.2590126991271973\n",
      "step : 6.17 % , loss : 2.0149118900299072\n",
      "step : 6.61 % , loss : 2.0918142795562744\n",
      "step : 7.05 % , loss : 2.0147223472595215\n",
      "step : 7.49 % , loss : 2.0118443965911865\n",
      "step : 7.93 % , loss : 2.1449320316314697\n",
      "step : 8.37 % , loss : 2.241868257522583\n",
      "step : 8.81 % , loss : 2.120973825454712\n",
      "step : 9.25 % , loss : 2.093250274658203\n",
      "step : 9.69 % , loss : 2.185901641845703\n",
      "step : 10.13 % , loss : 2.1668381690979004\n",
      "step : 10.57 % , loss : 2.2423629760742188\n",
      "step : 11.01 % , loss : 2.2221755981445312\n",
      "step : 11.45 % , loss : 2.0717580318450928\n",
      "step : 11.89 % , loss : 2.0680668354034424\n",
      "step : 12.33 % , loss : 2.1050233840942383\n",
      "step : 12.78 % , loss : 2.039860725402832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 13.22 % , loss : 2.0229721069335938\n",
      "step : 13.66 % , loss : 2.1024558544158936\n",
      "step : 14.1 % , loss : 2.136223793029785\n",
      "step : 14.54 % , loss : 2.2031219005584717\n",
      "step : 14.98 % , loss : 2.0916271209716797\n",
      "step : 15.42 % , loss : 2.1149911880493164\n",
      "step : 15.86 % , loss : 2.0195858478546143\n",
      "step : 16.3 % , loss : 2.138136386871338\n",
      "step : 16.74 % , loss : 2.056593418121338\n",
      "step : 17.18 % , loss : 2.093890428543091\n",
      "step : 17.62 % , loss : 2.1512436866760254\n",
      "step : 18.06 % , loss : 2.137345552444458\n",
      "step : 18.5 % , loss : 2.262291431427002\n",
      "step : 18.94 % , loss : 2.111815929412842\n",
      "step : 19.38 % , loss : 2.2616701126098633\n",
      "step : 19.82 % , loss : 2.089376211166382\n",
      "step : 20.26 % , loss : 2.1547341346740723\n",
      "step : 20.7 % , loss : 2.2561228275299072\n",
      "step : 21.15 % , loss : 2.148977041244507\n",
      "step : 21.59 % , loss : 2.141474962234497\n",
      "step : 22.03 % , loss : 1.9826527833938599\n",
      "step : 22.47 % , loss : 2.187077045440674\n",
      "step : 22.91 % , loss : 2.1756575107574463\n",
      "step : 23.35 % , loss : 2.1550240516662598\n",
      "step : 23.79 % , loss : 2.0872411727905273\n",
      "step : 24.23 % , loss : 2.08896541595459\n",
      "step : 24.67 % , loss : 2.1376724243164062\n",
      "step : 25.11 % , loss : 2.222471237182617\n",
      "step : 25.55 % , loss : 2.127728223800659\n",
      "step : 25.99 % , loss : 2.1931824684143066\n",
      "step : 26.43 % , loss : 2.164865732192993\n",
      "step : 26.87 % , loss : 2.2126519680023193\n",
      "step : 27.31 % , loss : 2.1420273780822754\n",
      "step : 27.75 % , loss : 2.104602575302124\n",
      "step : 28.19 % , loss : 2.121460199356079\n",
      "step : 28.63 % , loss : 2.1277451515197754\n",
      "step : 29.07 % , loss : 2.2635602951049805\n",
      "step : 29.52 % , loss : 2.2193806171417236\n",
      "step : 29.96 % , loss : 2.1343491077423096\n",
      "step : 30.4 % , loss : 2.0282583236694336\n",
      "step : 30.84 % , loss : 2.1491146087646484\n",
      "step : 31.28 % , loss : 2.103130340576172\n",
      "step : 31.72 % , loss : 2.073122978210449\n",
      "step : 32.16 % , loss : 1.9764378070831299\n",
      "step : 32.6 % , loss : 2.0378715991973877\n",
      "step : 33.04 % , loss : 2.233386993408203\n",
      "step : 33.48 % , loss : 2.181873321533203\n",
      "step : 33.92 % , loss : 2.198974370956421\n",
      "step : 34.36 % , loss : 2.0047643184661865\n",
      "step : 34.8 % , loss : 2.1623380184173584\n",
      "step : 35.24 % , loss : 2.19816517829895\n",
      "step : 35.68 % , loss : 2.076785087585449\n",
      "step : 36.12 % , loss : 2.063706159591675\n",
      "step : 36.56 % , loss : 2.070676565170288\n",
      "step : 37.0 % , loss : 2.2101705074310303\n",
      "step : 37.44 % , loss : 2.138481378555298\n",
      "step : 37.89 % , loss : 2.0705881118774414\n",
      "step : 38.33 % , loss : 2.0818042755126953\n",
      "step : 38.77 % , loss : 2.091153383255005\n",
      "step : 39.21 % , loss : 2.0075905323028564\n",
      "step : 39.65 % , loss : 2.082101345062256\n",
      "step : 40.09 % , loss : 2.165358304977417\n",
      "step : 40.53 % , loss : 2.0779497623443604\n",
      "step : 40.97 % , loss : 2.325066089630127\n",
      "step : 41.41 % , loss : 1.9741737842559814\n",
      "step : 41.85 % , loss : 2.2030014991760254\n",
      "step : 42.29 % , loss : 2.296058416366577\n",
      "step : 42.73 % , loss : 2.0593626499176025\n",
      "step : 43.17 % , loss : 1.914044976234436\n",
      "step : 43.61 % , loss : 1.9833273887634277\n",
      "step : 44.05 % , loss : 2.2115402221679688\n",
      "step : 44.49 % , loss : 2.169579267501831\n",
      "step : 44.93 % , loss : 2.2241628170013428\n",
      "step : 45.37 % , loss : 2.119731903076172\n",
      "step : 45.81 % , loss : 2.0777781009674072\n",
      "step : 46.26 % , loss : 2.109518051147461\n",
      "step : 46.7 % , loss : 2.0970559120178223\n",
      "step : 47.14 % , loss : 2.0726547241210938\n",
      "step : 47.58 % , loss : 2.0453858375549316\n",
      "step : 48.02 % , loss : 2.0612707138061523\n",
      "step : 48.46 % , loss : 2.0134263038635254\n",
      "step : 48.9 % , loss : 2.1527774333953857\n",
      "step : 49.34 % , loss : 2.0955708026885986\n",
      "step : 49.78 % , loss : 1.9736590385437012\n",
      "step : 50.22 % , loss : 2.2685892581939697\n",
      "step : 50.66 % , loss : 2.1911652088165283\n",
      "step : 51.1 % , loss : 2.057934045791626\n",
      "step : 51.54 % , loss : 2.131711006164551\n",
      "step : 51.98 % , loss : 2.1437549591064453\n",
      "step : 52.42 % , loss : 1.9945114850997925\n",
      "step : 52.86 % , loss : 2.1711082458496094\n",
      "step : 53.3 % , loss : 2.135073661804199\n",
      "step : 53.74 % , loss : 2.173792600631714\n",
      "step : 54.19 % , loss : 2.1226859092712402\n",
      "step : 54.63 % , loss : 2.249819040298462\n",
      "step : 55.07 % , loss : 2.065922260284424\n",
      "step : 55.51 % , loss : 2.140657424926758\n",
      "step : 55.95 % , loss : 2.0624501705169678\n",
      "step : 56.39 % , loss : 2.089817523956299\n",
      "step : 56.83 % , loss : 2.0932528972625732\n",
      "step : 57.27 % , loss : 2.2117245197296143\n",
      "step : 57.71 % , loss : 2.085353136062622\n",
      "step : 58.15 % , loss : 2.0818986892700195\n",
      "step : 58.59 % , loss : 2.11604642868042\n",
      "step : 59.03 % , loss : 2.0179033279418945\n",
      "step : 59.47 % , loss : 1.974460244178772\n",
      "step : 59.91 % , loss : 2.087985038757324\n",
      "step : 60.35 % , loss : 2.0648252964019775\n",
      "step : 60.79 % , loss : 2.1564218997955322\n",
      "step : 61.23 % , loss : 2.1906368732452393\n",
      "step : 61.67 % , loss : 2.045461416244507\n",
      "step : 62.11 % , loss : 2.1367385387420654\n",
      "step : 62.56 % , loss : 2.0141522884368896\n",
      "step : 63.0 % , loss : 2.162851572036743\n",
      "step : 63.44 % , loss : 2.2136504650115967\n",
      "step : 63.88 % , loss : 1.9932804107666016\n",
      "step : 64.32 % , loss : 2.167097330093384\n",
      "step : 64.76 % , loss : 2.066516399383545\n",
      "step : 65.2 % , loss : 2.039313793182373\n",
      "step : 65.64 % , loss : 2.0988094806671143\n",
      "step : 66.08 % , loss : 2.0458061695098877\n",
      "step : 66.52 % , loss : 2.1940438747406006\n",
      "step : 66.96 % , loss : 2.0592517852783203\n",
      "step : 67.4 % , loss : 2.0265350341796875\n",
      "step : 67.84 % , loss : 2.0573012828826904\n",
      "step : 68.28 % , loss : 1.9666999578475952\n",
      "step : 68.72 % , loss : 1.9758743047714233\n",
      "step : 69.16 % , loss : 2.0101892948150635\n",
      "step : 69.6 % , loss : 2.0212764739990234\n",
      "step : 70.04 % , loss : 2.1464195251464844\n",
      "step : 70.48 % , loss : 1.960727334022522\n",
      "step : 70.93 % , loss : 2.0980968475341797\n",
      "step : 71.37 % , loss : 2.1573705673217773\n",
      "step : 71.81 % , loss : 2.186647653579712\n",
      "step : 72.25 % , loss : 1.9942214488983154\n",
      "step : 72.69 % , loss : 2.187840700149536\n",
      "step : 73.13 % , loss : 2.0978057384490967\n",
      "step : 73.57 % , loss : 2.201525926589966\n",
      "step : 74.01 % , loss : 2.124145269393921\n",
      "step : 74.45 % , loss : 2.007547378540039\n",
      "step : 74.89 % , loss : 1.9508394002914429\n",
      "step : 75.33 % , loss : 2.0902507305145264\n",
      "step : 75.77 % , loss : 2.216341972351074\n",
      "step : 76.21 % , loss : 2.143918037414551\n",
      "step : 76.65 % , loss : 2.1236088275909424\n",
      "step : 77.09 % , loss : 2.05603289604187\n",
      "step : 77.53 % , loss : 2.128370761871338\n",
      "step : 77.97 % , loss : 2.009962320327759\n",
      "step : 78.41 % , loss : 1.9701980352401733\n",
      "step : 78.85 % , loss : 2.155219316482544\n",
      "step : 79.3 % , loss : 2.1164393424987793\n",
      "step : 79.74 % , loss : 2.1226162910461426\n",
      "step : 80.18 % , loss : 2.114004135131836\n",
      "step : 80.62 % , loss : 2.149878978729248\n",
      "step : 81.06 % , loss : 2.049677848815918\n",
      "step : 81.5 % , loss : 1.9493860006332397\n",
      "step : 81.94 % , loss : 2.2151317596435547\n",
      "step : 82.38 % , loss : 1.9719830751419067\n",
      "step : 82.82 % , loss : 2.12742018699646\n",
      "step : 83.26 % , loss : 2.222071647644043\n",
      "step : 83.7 % , loss : 2.034853219985962\n",
      "step : 84.14 % , loss : 2.2187042236328125\n",
      "step : 84.58 % , loss : 1.9836024045944214\n",
      "step : 85.02 % , loss : 2.1845576763153076\n",
      "step : 85.46 % , loss : 1.9871264696121216\n",
      "step : 85.9 % , loss : 2.142484426498413\n",
      "step : 86.34 % , loss : 2.1838080883026123\n",
      "step : 86.78 % , loss : 2.090230703353882\n",
      "step : 87.22 % , loss : 2.1618359088897705\n",
      "step : 87.67 % , loss : 2.007728338241577\n",
      "step : 88.11 % , loss : 2.146327018737793\n",
      "step : 88.55 % , loss : 2.1273863315582275\n",
      "step : 88.99 % , loss : 2.1395225524902344\n",
      "step : 89.43 % , loss : 2.021446466445923\n",
      "step : 89.87 % , loss : 2.107473134994507\n",
      "step : 90.31 % , loss : 1.936508297920227\n",
      "step : 90.75 % , loss : 2.071533441543579\n",
      "step : 91.19 % , loss : 2.1892290115356445\n",
      "step : 91.63 % , loss : 2.1097991466522217\n",
      "step : 92.07 % , loss : 1.991103172302246\n",
      "step : 92.51 % , loss : 2.159996747970581\n",
      "step : 92.95 % , loss : 2.162644863128662\n",
      "step : 93.39 % , loss : 2.0065433979034424\n",
      "step : 93.83 % , loss : 2.124612808227539\n",
      "step : 94.27 % , loss : 2.08353853225708\n",
      "step : 94.71 % , loss : 1.9776860475540161\n",
      "step : 95.15 % , loss : 2.067049980163574\n",
      "step : 95.59 % , loss : 2.0770766735076904\n",
      "step : 96.04 % , loss : 1.909561276435852\n",
      "step : 96.48 % , loss : 1.9970232248306274\n",
      "step : 96.92 % , loss : 1.8055214881896973\n",
      "step : 97.36 % , loss : 2.0137269496917725\n",
      "step : 97.8 % , loss : 2.145869493484497\n",
      "step : 98.24 % , loss : 2.2522618770599365\n",
      "step : 98.68 % , loss : 2.1675198078155518\n",
      "step : 99.12 % , loss : 2.099295139312744\n",
      "step : 99.56 % , loss : 2.063385248184204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0m 8s\n",
      "\tTrain Loss: 2.108 | Train PPL: 8.230\n",
      "\tValidation Loss: 1.985 | Validation PPL: 7.281\n",
      "step : 0.0 % , loss : 1.791021704673767\n",
      "step : 0.44 % , loss : 1.9374475479125977\n",
      "step : 0.88 % , loss : 1.7574045658111572\n",
      "step : 1.32 % , loss : 1.9116246700286865\n",
      "step : 1.76 % , loss : 1.9793123006820679\n",
      "step : 2.2 % , loss : 1.932409644126892\n",
      "step : 2.64 % , loss : 1.8902413845062256\n",
      "step : 3.08 % , loss : 2.011996030807495\n",
      "step : 3.52 % , loss : 1.8121767044067383\n",
      "step : 3.96 % , loss : 1.972497820854187\n",
      "step : 4.41 % , loss : 1.9084155559539795\n",
      "step : 4.85 % , loss : 2.012742280960083\n",
      "step : 5.29 % , loss : 2.073146343231201\n",
      "step : 5.73 % , loss : 1.9084762334823608\n",
      "step : 6.17 % , loss : 1.9319900274276733\n",
      "step : 6.61 % , loss : 1.887380599975586\n",
      "step : 7.05 % , loss : 2.0409748554229736\n",
      "step : 7.49 % , loss : 1.8498364686965942\n",
      "step : 7.93 % , loss : 1.9673994779586792\n",
      "step : 8.37 % , loss : 2.0053622722625732\n",
      "step : 8.81 % , loss : 2.0065128803253174\n",
      "step : 9.25 % , loss : 1.927141785621643\n",
      "step : 9.69 % , loss : 1.9565021991729736\n",
      "step : 10.13 % , loss : 1.9693113565444946\n",
      "step : 10.57 % , loss : 1.9617993831634521\n",
      "step : 11.01 % , loss : 1.953660488128662\n",
      "step : 11.45 % , loss : 1.9856832027435303\n",
      "step : 11.89 % , loss : 2.005855083465576\n",
      "step : 12.33 % , loss : 1.945672869682312\n",
      "step : 12.78 % , loss : 1.9302606582641602\n",
      "step : 13.22 % , loss : 2.018129587173462\n",
      "step : 13.66 % , loss : 1.8143951892852783\n",
      "step : 14.1 % , loss : 2.098546266555786\n",
      "step : 14.54 % , loss : 1.9023234844207764\n",
      "step : 14.98 % , loss : 1.9249097108840942\n",
      "step : 15.42 % , loss : 1.8900485038757324\n",
      "step : 15.86 % , loss : 1.9566764831542969\n",
      "step : 16.3 % , loss : 1.967580795288086\n",
      "step : 16.74 % , loss : 1.9761906862258911\n",
      "step : 17.18 % , loss : 1.8596274852752686\n",
      "step : 17.62 % , loss : 1.9158546924591064\n",
      "step : 18.06 % , loss : 1.999027967453003\n",
      "step : 18.5 % , loss : 1.896659016609192\n",
      "step : 18.94 % , loss : 2.002469539642334\n",
      "step : 19.38 % , loss : 1.9234706163406372\n",
      "step : 19.82 % , loss : 1.976645827293396\n",
      "step : 20.26 % , loss : 1.9191726446151733\n",
      "step : 20.7 % , loss : 1.9001938104629517\n",
      "step : 21.15 % , loss : 1.9874024391174316\n",
      "step : 21.59 % , loss : 1.9329777956008911\n",
      "step : 22.03 % , loss : 2.162402868270874\n",
      "step : 22.47 % , loss : 2.064049482345581\n",
      "step : 22.91 % , loss : 1.863607406616211\n",
      "step : 23.35 % , loss : 1.8478392362594604\n",
      "step : 23.79 % , loss : 1.9364949464797974\n",
      "step : 24.23 % , loss : 1.926755428314209\n",
      "step : 24.67 % , loss : 2.018610715866089\n",
      "step : 25.11 % , loss : 1.947036862373352\n",
      "step : 25.55 % , loss : 1.9555902481079102\n",
      "step : 25.99 % , loss : 1.9905263185501099\n",
      "step : 26.43 % , loss : 2.015070915222168\n",
      "step : 26.87 % , loss : 1.9487768411636353\n",
      "step : 27.31 % , loss : 1.9849389791488647\n",
      "step : 27.75 % , loss : 2.1396002769470215\n",
      "step : 28.19 % , loss : 2.020352602005005\n",
      "step : 28.63 % , loss : 1.9699019193649292\n",
      "step : 29.07 % , loss : 1.9813787937164307\n",
      "step : 29.52 % , loss : 2.0455942153930664\n",
      "step : 29.96 % , loss : 1.7699956893920898\n",
      "step : 30.4 % , loss : 1.8285051584243774\n",
      "step : 30.84 % , loss : 1.8733017444610596\n",
      "step : 31.28 % , loss : 2.0200722217559814\n",
      "step : 31.72 % , loss : 2.0072131156921387\n",
      "step : 32.16 % , loss : 1.9762568473815918\n",
      "step : 32.6 % , loss : 2.0653560161590576\n",
      "step : 33.04 % , loss : 2.012702465057373\n",
      "step : 33.48 % , loss : 1.8742280006408691\n",
      "step : 33.92 % , loss : 2.0007407665252686\n",
      "step : 34.36 % , loss : 1.9040913581848145\n",
      "step : 34.8 % , loss : 2.0275166034698486\n",
      "step : 35.24 % , loss : 1.8663318157196045\n",
      "step : 35.68 % , loss : 1.917798399925232\n",
      "step : 36.12 % , loss : 2.030836343765259\n",
      "step : 36.56 % , loss : 1.8470574617385864\n",
      "step : 37.0 % , loss : 2.0055079460144043\n",
      "step : 37.44 % , loss : 1.891040563583374\n",
      "step : 37.89 % , loss : 1.9384413957595825\n",
      "step : 38.33 % , loss : 1.89741849899292\n",
      "step : 38.77 % , loss : 2.000574827194214\n",
      "step : 39.21 % , loss : 1.9787720441818237\n",
      "step : 39.65 % , loss : 1.9835009574890137\n",
      "step : 40.09 % , loss : 1.9530119895935059\n",
      "step : 40.53 % , loss : 1.8598672151565552\n",
      "step : 40.97 % , loss : 1.9298762083053589\n",
      "step : 41.41 % , loss : 1.8869277238845825\n",
      "step : 41.85 % , loss : 1.9911143779754639\n",
      "step : 42.29 % , loss : 1.9430644512176514\n",
      "step : 42.73 % , loss : 1.7940582036972046\n",
      "step : 43.17 % , loss : 1.877254605293274\n",
      "step : 43.61 % , loss : 1.9037011861801147\n",
      "step : 44.05 % , loss : 1.823886752128601\n",
      "step : 44.49 % , loss : 1.8980753421783447\n",
      "step : 44.93 % , loss : 2.012821674346924\n",
      "step : 45.37 % , loss : 2.0077786445617676\n",
      "step : 45.81 % , loss : 1.8061784505844116\n",
      "step : 46.26 % , loss : 1.9170750379562378\n",
      "step : 46.7 % , loss : 1.779119610786438\n",
      "step : 47.14 % , loss : 1.9501813650131226\n",
      "step : 47.58 % , loss : 1.9111690521240234\n",
      "step : 48.02 % , loss : 1.8847006559371948\n",
      "step : 48.46 % , loss : 2.0031533241271973\n",
      "step : 48.9 % , loss : 1.9906736612319946\n",
      "step : 49.34 % , loss : 1.9731014966964722\n",
      "step : 49.78 % , loss : 2.044168710708618\n",
      "step : 50.22 % , loss : 1.96640145778656\n",
      "step : 50.66 % , loss : 1.8415403366088867\n",
      "step : 51.1 % , loss : 1.8351083993911743\n",
      "step : 51.54 % , loss : 1.7964577674865723\n",
      "step : 51.98 % , loss : 1.8352490663528442\n",
      "step : 52.42 % , loss : 1.958648681640625\n",
      "step : 52.86 % , loss : 1.8365116119384766\n",
      "step : 53.3 % , loss : 1.9416050910949707\n",
      "step : 53.74 % , loss : 1.9830297231674194\n",
      "step : 54.19 % , loss : 2.0271406173706055\n",
      "step : 54.63 % , loss : 1.934313416481018\n",
      "step : 55.07 % , loss : 1.981855869293213\n",
      "step : 55.51 % , loss : 1.946803092956543\n",
      "step : 55.95 % , loss : 1.832687258720398\n",
      "step : 56.39 % , loss : 2.043173313140869\n",
      "step : 56.83 % , loss : 1.9048411846160889\n",
      "step : 57.27 % , loss : 1.9196033477783203\n",
      "step : 57.71 % , loss : 1.953554391860962\n",
      "step : 58.15 % , loss : 1.846516489982605\n",
      "step : 58.59 % , loss : 1.9679994583129883\n",
      "step : 59.03 % , loss : 1.9995577335357666\n",
      "step : 59.47 % , loss : 1.837481141090393\n",
      "step : 59.91 % , loss : 1.9528342485427856\n",
      "step : 60.35 % , loss : 1.8152252435684204\n",
      "step : 60.79 % , loss : 1.8760106563568115\n",
      "step : 61.23 % , loss : 2.1801230907440186\n",
      "step : 61.67 % , loss : 2.0364997386932373\n",
      "step : 62.11 % , loss : 1.8954589366912842\n",
      "step : 62.56 % , loss : 1.942145824432373\n",
      "step : 63.0 % , loss : 1.8691766262054443\n",
      "step : 63.44 % , loss : 1.8504918813705444\n",
      "step : 63.88 % , loss : 1.9335392713546753\n",
      "step : 64.32 % , loss : 1.90462327003479\n",
      "step : 64.76 % , loss : 1.9023030996322632\n",
      "step : 65.2 % , loss : 1.8584372997283936\n",
      "step : 65.64 % , loss : 1.8033372163772583\n",
      "step : 66.08 % , loss : 1.8992012739181519\n",
      "step : 66.52 % , loss : 2.0379419326782227\n",
      "step : 66.96 % , loss : 2.00628399848938\n",
      "step : 67.4 % , loss : 2.0417189598083496\n",
      "step : 67.84 % , loss : 2.0377018451690674\n",
      "step : 68.28 % , loss : 1.894256353378296\n",
      "step : 68.72 % , loss : 2.1859264373779297\n",
      "step : 69.16 % , loss : 1.890754222869873\n",
      "step : 69.6 % , loss : 2.0552408695220947\n",
      "step : 70.04 % , loss : 1.9687223434448242\n",
      "step : 70.48 % , loss : 1.896451473236084\n",
      "step : 70.93 % , loss : 2.0468599796295166\n",
      "step : 71.37 % , loss : 1.9354394674301147\n",
      "step : 71.81 % , loss : 1.8864383697509766\n",
      "step : 72.25 % , loss : 2.0636227130889893\n",
      "step : 72.69 % , loss : 1.8578531742095947\n",
      "step : 73.13 % , loss : 1.838054895401001\n",
      "step : 73.57 % , loss : 1.993523359298706\n",
      "step : 74.01 % , loss : 1.925147533416748\n",
      "step : 74.45 % , loss : 2.0744593143463135\n",
      "step : 74.89 % , loss : 1.9690831899642944\n",
      "step : 75.33 % , loss : 1.9845058917999268\n",
      "step : 75.77 % , loss : 1.892566442489624\n",
      "step : 76.21 % , loss : 2.082268714904785\n",
      "step : 76.65 % , loss : 1.981094241142273\n",
      "step : 77.09 % , loss : 1.9164328575134277\n",
      "step : 77.53 % , loss : 1.9374756813049316\n",
      "step : 77.97 % , loss : 1.9222468137741089\n",
      "step : 78.41 % , loss : 1.9103704690933228\n",
      "step : 78.85 % , loss : 1.8171699047088623\n",
      "step : 79.3 % , loss : 2.1194002628326416\n",
      "step : 79.74 % , loss : 1.880381464958191\n",
      "step : 80.18 % , loss : 1.9160012006759644\n",
      "step : 80.62 % , loss : 1.958151936531067\n",
      "step : 81.06 % , loss : 1.8803091049194336\n",
      "step : 81.5 % , loss : 1.9468040466308594\n",
      "step : 81.94 % , loss : 1.8378163576126099\n",
      "step : 82.38 % , loss : 1.7463319301605225\n",
      "step : 82.82 % , loss : 2.05279803276062\n",
      "step : 83.26 % , loss : 1.8723242282867432\n",
      "step : 83.7 % , loss : 1.9957069158554077\n",
      "step : 84.14 % , loss : 1.991142988204956\n",
      "step : 84.58 % , loss : 2.057859182357788\n",
      "step : 85.02 % , loss : 1.9664921760559082\n",
      "step : 85.46 % , loss : 2.0562870502471924\n",
      "step : 85.9 % , loss : 1.907353401184082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 86.34 % , loss : 1.977333664894104\n",
      "step : 86.78 % , loss : 1.745039463043213\n",
      "step : 87.22 % , loss : 1.8117177486419678\n",
      "step : 87.67 % , loss : 1.9380615949630737\n",
      "step : 88.11 % , loss : 1.988111138343811\n",
      "step : 88.55 % , loss : 1.9332693815231323\n",
      "step : 88.99 % , loss : 1.9654865264892578\n",
      "step : 89.43 % , loss : 1.8776130676269531\n",
      "step : 89.87 % , loss : 2.004777193069458\n",
      "step : 90.31 % , loss : 1.9109052419662476\n",
      "step : 90.75 % , loss : 1.857208251953125\n",
      "step : 91.19 % , loss : 1.9543542861938477\n",
      "step : 91.63 % , loss : 1.9462770223617554\n",
      "step : 92.07 % , loss : 2.021052598953247\n",
      "step : 92.51 % , loss : 1.913454294204712\n",
      "step : 92.95 % , loss : 2.0148308277130127\n",
      "step : 93.39 % , loss : 1.8390083312988281\n",
      "step : 93.83 % , loss : 1.9096697568893433\n",
      "step : 94.27 % , loss : 1.8311681747436523\n",
      "step : 94.71 % , loss : 2.058652877807617\n",
      "step : 95.15 % , loss : 1.858422875404358\n",
      "step : 95.59 % , loss : 1.9519941806793213\n",
      "step : 96.04 % , loss : 1.9052109718322754\n",
      "step : 96.48 % , loss : 1.828778624534607\n",
      "step : 96.92 % , loss : 1.8968526124954224\n",
      "step : 97.36 % , loss : 1.8588565587997437\n",
      "step : 97.8 % , loss : 1.9534854888916016\n",
      "step : 98.24 % , loss : 1.9902217388153076\n",
      "step : 98.68 % , loss : 1.9474213123321533\n",
      "step : 99.12 % , loss : 1.797088861465454\n",
      "step : 99.56 % , loss : 1.853298544883728\n",
      "Epoch: 07 | Time: 0m 8s\n",
      "\tTrain Loss: 1.939 | Train PPL: 6.955\n",
      "\tValidation Loss: 1.889 | Validation PPL: 6.614\n",
      "step : 0.0 % , loss : 1.7753372192382812\n",
      "step : 0.44 % , loss : 1.7259092330932617\n",
      "step : 0.88 % , loss : 1.888634443283081\n",
      "step : 1.32 % , loss : 1.7299894094467163\n",
      "step : 1.76 % , loss : 1.790797472000122\n",
      "step : 2.2 % , loss : 1.7208564281463623\n",
      "step : 2.64 % , loss : 1.836627721786499\n",
      "step : 3.08 % , loss : 1.835808277130127\n",
      "step : 3.52 % , loss : 1.9032564163208008\n",
      "step : 3.96 % , loss : 1.828155279159546\n",
      "step : 4.41 % , loss : 1.763968586921692\n",
      "step : 4.85 % , loss : 1.711963415145874\n",
      "step : 5.29 % , loss : 1.818042516708374\n",
      "step : 5.73 % , loss : 1.875733733177185\n",
      "step : 6.17 % , loss : 1.7053520679473877\n",
      "step : 6.61 % , loss : 1.9170207977294922\n",
      "step : 7.05 % , loss : 1.7204848527908325\n",
      "step : 7.49 % , loss : 1.7046452760696411\n",
      "step : 7.93 % , loss : 1.8313077688217163\n",
      "step : 8.37 % , loss : 1.6213918924331665\n",
      "step : 8.81 % , loss : 1.6682928800582886\n",
      "step : 9.25 % , loss : 1.7424198389053345\n",
      "step : 9.69 % , loss : 1.9209914207458496\n",
      "step : 10.13 % , loss : 1.9553924798965454\n",
      "step : 10.57 % , loss : 1.855621337890625\n",
      "step : 11.01 % , loss : 1.6883606910705566\n",
      "step : 11.45 % , loss : 1.9602560997009277\n",
      "step : 11.89 % , loss : 1.8248180150985718\n",
      "step : 12.33 % , loss : 1.761699914932251\n",
      "step : 12.78 % , loss : 1.8761451244354248\n",
      "step : 13.22 % , loss : 1.8744972944259644\n",
      "step : 13.66 % , loss : 1.8414463996887207\n",
      "step : 14.1 % , loss : 1.874078631401062\n",
      "step : 14.54 % , loss : 1.8636572360992432\n",
      "step : 14.98 % , loss : 1.8348808288574219\n",
      "step : 15.42 % , loss : 1.8338807821273804\n",
      "step : 15.86 % , loss : 1.8443145751953125\n",
      "step : 16.3 % , loss : 1.741928219795227\n",
      "step : 16.74 % , loss : 1.6825647354125977\n",
      "step : 17.18 % , loss : 1.7337597608566284\n",
      "step : 17.62 % , loss : 1.7634669542312622\n",
      "step : 18.06 % , loss : 1.7875131368637085\n",
      "step : 18.5 % , loss : 1.7195216417312622\n",
      "step : 18.94 % , loss : 1.8495365381240845\n",
      "step : 19.38 % , loss : 1.8117855787277222\n",
      "step : 19.82 % , loss : 1.9049575328826904\n",
      "step : 20.26 % , loss : 1.7743346691131592\n",
      "step : 20.7 % , loss : 1.6251444816589355\n",
      "step : 21.15 % , loss : 1.7223126888275146\n",
      "step : 21.59 % , loss : 1.8918876647949219\n",
      "step : 22.03 % , loss : 1.9906810522079468\n",
      "step : 22.47 % , loss : 1.7616993188858032\n",
      "step : 22.91 % , loss : 1.9406338930130005\n",
      "step : 23.35 % , loss : 1.8668618202209473\n",
      "step : 23.79 % , loss : 1.7939647436141968\n",
      "step : 24.23 % , loss : 1.811363935470581\n",
      "step : 24.67 % , loss : 1.9414284229278564\n",
      "step : 25.11 % , loss : 1.7175931930541992\n",
      "step : 25.55 % , loss : 1.7717100381851196\n",
      "step : 25.99 % , loss : 1.7506914138793945\n",
      "step : 26.43 % , loss : 1.7947301864624023\n",
      "step : 26.87 % , loss : 1.7278794050216675\n",
      "step : 27.31 % , loss : 1.7337682247161865\n",
      "step : 27.75 % , loss : 1.8170260190963745\n",
      "step : 28.19 % , loss : 1.861578345298767\n",
      "step : 28.63 % , loss : 1.7402228116989136\n",
      "step : 29.07 % , loss : 1.97359299659729\n",
      "step : 29.52 % , loss : 1.7383291721343994\n",
      "step : 29.96 % , loss : 1.835744023323059\n",
      "step : 30.4 % , loss : 1.801458477973938\n",
      "step : 30.84 % , loss : 1.6266534328460693\n",
      "step : 31.28 % , loss : 1.6767616271972656\n",
      "step : 31.72 % , loss : 1.8902137279510498\n",
      "step : 32.16 % , loss : 1.7941147089004517\n",
      "step : 32.6 % , loss : 1.8587861061096191\n",
      "step : 33.04 % , loss : 1.7169582843780518\n",
      "step : 33.48 % , loss : 1.8362057209014893\n",
      "step : 33.92 % , loss : 1.794678807258606\n",
      "step : 34.36 % , loss : 1.5570404529571533\n",
      "step : 34.8 % , loss : 1.9808377027511597\n",
      "step : 35.24 % , loss : 1.7599989175796509\n",
      "step : 35.68 % , loss : 1.7330856323242188\n",
      "step : 36.12 % , loss : 1.7405340671539307\n",
      "step : 36.56 % , loss : 1.8181442022323608\n",
      "step : 37.0 % , loss : 1.8080462217330933\n",
      "step : 37.44 % , loss : 1.730724811553955\n",
      "step : 37.89 % , loss : 1.822064995765686\n",
      "step : 38.33 % , loss : 1.8447556495666504\n",
      "step : 38.77 % , loss : 1.7094132900238037\n",
      "step : 39.21 % , loss : 1.8187286853790283\n",
      "step : 39.65 % , loss : 1.744146704673767\n",
      "step : 40.09 % , loss : 1.7251776456832886\n",
      "step : 40.53 % , loss : 1.855160117149353\n",
      "step : 40.97 % , loss : 1.775998592376709\n",
      "step : 41.41 % , loss : 1.8125027418136597\n",
      "step : 41.85 % , loss : 1.8164664506912231\n",
      "step : 42.29 % , loss : 1.7162941694259644\n",
      "step : 42.73 % , loss : 1.7986136674880981\n",
      "step : 43.17 % , loss : 1.863028645515442\n",
      "step : 43.61 % , loss : 1.7968339920043945\n",
      "step : 44.05 % , loss : 1.7475581169128418\n",
      "step : 44.49 % , loss : 1.721268653869629\n",
      "step : 44.93 % , loss : 1.669913411140442\n",
      "step : 45.37 % , loss : 1.7933157682418823\n",
      "step : 45.81 % , loss : 1.8121038675308228\n",
      "step : 46.26 % , loss : 1.672707200050354\n",
      "step : 46.7 % , loss : 1.7866088151931763\n",
      "step : 47.14 % , loss : 1.7615045309066772\n",
      "step : 47.58 % , loss : 1.9516444206237793\n",
      "step : 48.02 % , loss : 1.7423473596572876\n",
      "step : 48.46 % , loss : 1.8824294805526733\n",
      "step : 48.9 % , loss : 1.831447958946228\n",
      "step : 49.34 % , loss : 1.81903874874115\n",
      "step : 49.78 % , loss : 1.7466249465942383\n",
      "step : 50.22 % , loss : 1.8500020503997803\n",
      "step : 50.66 % , loss : 1.7880733013153076\n",
      "step : 51.1 % , loss : 1.8787713050842285\n",
      "step : 51.54 % , loss : 1.679532527923584\n",
      "step : 51.98 % , loss : 1.8215086460113525\n",
      "step : 52.42 % , loss : 1.7365915775299072\n",
      "step : 52.86 % , loss : 1.7307465076446533\n",
      "step : 53.3 % , loss : 1.6928714513778687\n",
      "step : 53.74 % , loss : 1.7423655986785889\n",
      "step : 54.19 % , loss : 1.6534488201141357\n",
      "step : 54.63 % , loss : 1.8802958726882935\n",
      "step : 55.07 % , loss : 1.8470408916473389\n",
      "step : 55.51 % , loss : 1.8585307598114014\n",
      "step : 55.95 % , loss : 1.9272114038467407\n",
      "step : 56.39 % , loss : 1.7162139415740967\n",
      "step : 56.83 % , loss : 1.7620426416397095\n",
      "step : 57.27 % , loss : 1.7939739227294922\n",
      "step : 57.71 % , loss : 1.8586176633834839\n",
      "step : 58.15 % , loss : 1.8536819219589233\n",
      "step : 58.59 % , loss : 1.8375192880630493\n",
      "step : 59.03 % , loss : 1.786014199256897\n",
      "step : 59.47 % , loss : 1.7857956886291504\n",
      "step : 59.91 % , loss : 1.981940746307373\n",
      "step : 60.35 % , loss : 1.8620004653930664\n",
      "step : 60.79 % , loss : 1.7914901971817017\n",
      "step : 61.23 % , loss : 1.7430602312088013\n",
      "step : 61.67 % , loss : 1.6233351230621338\n",
      "step : 62.11 % , loss : 1.8801716566085815\n",
      "step : 62.56 % , loss : 1.8644760847091675\n",
      "step : 63.0 % , loss : 1.7685540914535522\n",
      "step : 63.44 % , loss : 1.8346704244613647\n",
      "step : 63.88 % , loss : 1.78570556640625\n",
      "step : 64.32 % , loss : 1.6657683849334717\n",
      "step : 64.76 % , loss : 1.7632646560668945\n",
      "step : 65.2 % , loss : 1.8417863845825195\n",
      "step : 65.64 % , loss : 1.7458014488220215\n",
      "step : 66.08 % , loss : 1.6785098314285278\n",
      "step : 66.52 % , loss : 1.7310497760772705\n",
      "step : 66.96 % , loss : 1.840622067451477\n",
      "step : 67.4 % , loss : 2.002941846847534\n",
      "step : 67.84 % , loss : 1.7850877046585083\n",
      "step : 68.28 % , loss : 1.856080412864685\n",
      "step : 68.72 % , loss : 1.910483717918396\n",
      "step : 69.16 % , loss : 1.7593142986297607\n",
      "step : 69.6 % , loss : 1.713882327079773\n",
      "step : 70.04 % , loss : 1.7816377878189087\n",
      "step : 70.48 % , loss : 1.805572509765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 70.93 % , loss : 1.8446755409240723\n",
      "step : 71.37 % , loss : 1.6834428310394287\n",
      "step : 71.81 % , loss : 1.857479453086853\n",
      "step : 72.25 % , loss : 1.7722527980804443\n",
      "step : 72.69 % , loss : 1.9396270513534546\n",
      "step : 73.13 % , loss : 1.8946176767349243\n",
      "step : 73.57 % , loss : 1.8714238405227661\n",
      "step : 74.01 % , loss : 1.8350355625152588\n",
      "step : 74.45 % , loss : 1.7523598670959473\n",
      "step : 74.89 % , loss : 1.7938815355300903\n",
      "step : 75.33 % , loss : 1.794954776763916\n",
      "step : 75.77 % , loss : 1.8068286180496216\n",
      "step : 76.21 % , loss : 1.7856336832046509\n",
      "step : 76.65 % , loss : 1.976407766342163\n",
      "step : 77.09 % , loss : 1.842067003250122\n",
      "step : 77.53 % , loss : 1.6985212564468384\n",
      "step : 77.97 % , loss : 1.7602702379226685\n",
      "step : 78.41 % , loss : 1.9434304237365723\n",
      "step : 78.85 % , loss : 1.8620011806488037\n",
      "step : 79.3 % , loss : 1.63624906539917\n",
      "step : 79.74 % , loss : 1.8865019083023071\n",
      "step : 80.18 % , loss : 1.944370985031128\n",
      "step : 80.62 % , loss : 1.7525254487991333\n",
      "step : 81.06 % , loss : 1.8004344701766968\n",
      "step : 81.5 % , loss : 1.861053705215454\n",
      "step : 81.94 % , loss : 1.7878446578979492\n",
      "step : 82.38 % , loss : 1.8711042404174805\n",
      "step : 82.82 % , loss : 1.8878304958343506\n",
      "step : 83.26 % , loss : 1.671665072441101\n",
      "step : 83.7 % , loss : 1.7752265930175781\n",
      "step : 84.14 % , loss : 1.7927851676940918\n",
      "step : 84.58 % , loss : 1.7700505256652832\n",
      "step : 85.02 % , loss : 1.7899794578552246\n",
      "step : 85.46 % , loss : 1.8263648748397827\n",
      "step : 85.9 % , loss : 1.6659342050552368\n",
      "step : 86.34 % , loss : 1.9132970571517944\n",
      "step : 86.78 % , loss : 1.7408208847045898\n",
      "step : 87.22 % , loss : 1.7017393112182617\n",
      "step : 87.67 % , loss : 1.7895349264144897\n",
      "step : 88.11 % , loss : 1.8718351125717163\n",
      "step : 88.55 % , loss : 1.6467539072036743\n",
      "step : 88.99 % , loss : 1.8453224897384644\n",
      "step : 89.43 % , loss : 1.7983462810516357\n",
      "step : 89.87 % , loss : 1.753832221031189\n",
      "step : 90.31 % , loss : 1.7363766431808472\n",
      "step : 90.75 % , loss : 1.7565503120422363\n",
      "step : 91.19 % , loss : 1.7759814262390137\n",
      "step : 91.63 % , loss : 1.575731635093689\n",
      "step : 92.07 % , loss : 1.771823763847351\n",
      "step : 92.51 % , loss : 1.7455062866210938\n",
      "step : 92.95 % , loss : 1.8232444524765015\n",
      "step : 93.39 % , loss : 1.7692351341247559\n",
      "step : 93.83 % , loss : 1.741493582725525\n",
      "step : 94.27 % , loss : 1.6254820823669434\n",
      "step : 94.71 % , loss : 1.779929757118225\n",
      "step : 95.15 % , loss : 1.6926794052124023\n",
      "step : 95.59 % , loss : 1.9096890687942505\n",
      "step : 96.04 % , loss : 1.8718494176864624\n",
      "step : 96.48 % , loss : 1.7792487144470215\n",
      "step : 96.92 % , loss : 1.8383185863494873\n",
      "step : 97.36 % , loss : 1.7521564960479736\n",
      "step : 97.8 % , loss : 1.6924495697021484\n",
      "step : 98.24 % , loss : 1.9255489110946655\n",
      "step : 98.68 % , loss : 1.6651884317398071\n",
      "step : 99.12 % , loss : 1.8415251970291138\n",
      "step : 99.56 % , loss : 1.7533899545669556\n",
      "Epoch: 08 | Time: 0m 8s\n",
      "\tTrain Loss: 1.795 | Train PPL: 6.020\n",
      "\tValidation Loss: 1.824 | Validation PPL: 6.194\n",
      "step : 0.0 % , loss : 1.619581699371338\n",
      "step : 0.44 % , loss : 1.521598219871521\n",
      "step : 0.88 % , loss : 1.7103703022003174\n",
      "step : 1.32 % , loss : 1.6966545581817627\n",
      "step : 1.76 % , loss : 1.8172720670700073\n",
      "step : 2.2 % , loss : 1.6880074739456177\n",
      "step : 2.64 % , loss : 1.8409749269485474\n",
      "step : 3.08 % , loss : 1.5614640712738037\n",
      "step : 3.52 % , loss : 1.7960962057113647\n",
      "step : 3.96 % , loss : 1.6700352430343628\n",
      "step : 4.41 % , loss : 1.641126036643982\n",
      "step : 4.85 % , loss : 1.7762370109558105\n",
      "step : 5.29 % , loss : 1.6527037620544434\n",
      "step : 5.73 % , loss : 1.6932626962661743\n",
      "step : 6.17 % , loss : 1.6520545482635498\n",
      "step : 6.61 % , loss : 1.6180964708328247\n",
      "step : 7.05 % , loss : 1.6848561763763428\n",
      "step : 7.49 % , loss : 1.6625399589538574\n",
      "step : 7.93 % , loss : 1.7755801677703857\n",
      "step : 8.37 % , loss : 1.6769945621490479\n",
      "step : 8.81 % , loss : 1.5773056745529175\n",
      "step : 9.25 % , loss : 1.6506799459457397\n",
      "step : 9.69 % , loss : 1.6850870847702026\n",
      "step : 10.13 % , loss : 1.7181434631347656\n",
      "step : 10.57 % , loss : 1.7674102783203125\n",
      "step : 11.01 % , loss : 1.620111107826233\n",
      "step : 11.45 % , loss : 1.6665080785751343\n",
      "step : 11.89 % , loss : 1.5908303260803223\n",
      "step : 12.33 % , loss : 1.737293004989624\n",
      "step : 12.78 % , loss : 1.6453397274017334\n",
      "step : 13.22 % , loss : 1.6939581632614136\n",
      "step : 13.66 % , loss : 1.6281771659851074\n",
      "step : 14.1 % , loss : 1.69395911693573\n",
      "step : 14.54 % , loss : 1.635363221168518\n",
      "step : 14.98 % , loss : 1.6578437089920044\n",
      "step : 15.42 % , loss : 1.75002121925354\n",
      "step : 15.86 % , loss : 1.7146189212799072\n",
      "step : 16.3 % , loss : 1.6788376569747925\n",
      "step : 16.74 % , loss : 1.7302607297897339\n",
      "step : 17.18 % , loss : 1.5390129089355469\n",
      "step : 17.62 % , loss : 1.6258318424224854\n",
      "step : 18.06 % , loss : 1.723106026649475\n",
      "step : 18.5 % , loss : 1.6692527532577515\n",
      "step : 18.94 % , loss : 1.7971867322921753\n",
      "step : 19.38 % , loss : 1.6224493980407715\n",
      "step : 19.82 % , loss : 1.723740577697754\n",
      "step : 20.26 % , loss : 1.714869737625122\n",
      "step : 20.7 % , loss : 1.7060506343841553\n",
      "step : 21.15 % , loss : 1.6818562746047974\n",
      "step : 21.59 % , loss : 1.6065386533737183\n",
      "step : 22.03 % , loss : 1.8021067380905151\n",
      "step : 22.47 % , loss : 1.6750565767288208\n",
      "step : 22.91 % , loss : 1.6101902723312378\n",
      "step : 23.35 % , loss : 1.7415095567703247\n",
      "step : 23.79 % , loss : 1.5829808712005615\n",
      "step : 24.23 % , loss : 1.6761585474014282\n",
      "step : 24.67 % , loss : 1.6561394929885864\n",
      "step : 25.11 % , loss : 1.6682302951812744\n",
      "step : 25.55 % , loss : 1.6035195589065552\n",
      "step : 25.99 % , loss : 1.7089955806732178\n",
      "step : 26.43 % , loss : 1.753392219543457\n",
      "step : 26.87 % , loss : 1.6392185688018799\n",
      "step : 27.31 % , loss : 1.5765280723571777\n",
      "step : 27.75 % , loss : 1.785754919052124\n",
      "step : 28.19 % , loss : 1.7373496294021606\n",
      "step : 28.63 % , loss : 1.787597894668579\n",
      "step : 29.07 % , loss : 1.6316367387771606\n",
      "step : 29.52 % , loss : 1.4867104291915894\n",
      "step : 29.96 % , loss : 1.6814000606536865\n",
      "step : 30.4 % , loss : 1.6492937803268433\n",
      "step : 30.84 % , loss : 1.773091435432434\n",
      "step : 31.28 % , loss : 1.637162685394287\n",
      "step : 31.72 % , loss : 1.7614036798477173\n",
      "step : 32.16 % , loss : 1.6471084356307983\n",
      "step : 32.6 % , loss : 1.6565525531768799\n",
      "step : 33.04 % , loss : 1.7528835535049438\n",
      "step : 33.48 % , loss : 1.648478627204895\n",
      "step : 33.92 % , loss : 1.5649864673614502\n",
      "step : 34.36 % , loss : 1.6249306201934814\n",
      "step : 34.8 % , loss : 1.6381018161773682\n",
      "step : 35.24 % , loss : 1.5831258296966553\n",
      "step : 35.68 % , loss : 1.530935287475586\n",
      "step : 36.12 % , loss : 1.5892013311386108\n",
      "step : 36.56 % , loss : 1.7188901901245117\n",
      "step : 37.0 % , loss : 1.5899149179458618\n",
      "step : 37.44 % , loss : 1.8318575620651245\n",
      "step : 37.89 % , loss : 1.6318652629852295\n",
      "step : 38.33 % , loss : 1.6557546854019165\n",
      "step : 38.77 % , loss : 1.6908620595932007\n",
      "step : 39.21 % , loss : 1.6974806785583496\n",
      "step : 39.65 % , loss : 1.6703795194625854\n",
      "step : 40.09 % , loss : 1.6631633043289185\n",
      "step : 40.53 % , loss : 1.714272379875183\n",
      "step : 40.97 % , loss : 1.6967687606811523\n",
      "step : 41.41 % , loss : 1.559544324874878\n",
      "step : 41.85 % , loss : 1.6024386882781982\n",
      "step : 42.29 % , loss : 1.7537071704864502\n",
      "step : 42.73 % , loss : 1.6559319496154785\n",
      "step : 43.17 % , loss : 1.5827916860580444\n",
      "step : 43.61 % , loss : 1.6817811727523804\n",
      "step : 44.05 % , loss : 1.6779437065124512\n",
      "step : 44.49 % , loss : 1.6741018295288086\n",
      "step : 44.93 % , loss : 1.5167648792266846\n",
      "step : 45.37 % , loss : 1.7072906494140625\n",
      "step : 45.81 % , loss : 1.6158348321914673\n",
      "step : 46.26 % , loss : 1.6954231262207031\n",
      "step : 46.7 % , loss : 1.72908616065979\n",
      "step : 47.14 % , loss : 1.665443778038025\n",
      "step : 47.58 % , loss : 1.531983494758606\n",
      "step : 48.02 % , loss : 1.7921291589736938\n",
      "step : 48.46 % , loss : 1.7291946411132812\n",
      "step : 48.9 % , loss : 1.6305118799209595\n",
      "step : 49.34 % , loss : 1.6683269739151\n",
      "step : 49.78 % , loss : 1.6895370483398438\n",
      "step : 50.22 % , loss : 1.6616928577423096\n",
      "step : 50.66 % , loss : 1.7662827968597412\n",
      "step : 51.1 % , loss : 1.6362991333007812\n",
      "step : 51.54 % , loss : 1.73771333694458\n",
      "step : 51.98 % , loss : 1.7539602518081665\n",
      "step : 52.42 % , loss : 1.6934291124343872\n",
      "step : 52.86 % , loss : 1.6525790691375732\n",
      "step : 53.3 % , loss : 1.5341084003448486\n",
      "step : 53.74 % , loss : 1.6316125392913818\n",
      "step : 54.19 % , loss : 1.635867714881897\n",
      "step : 54.63 % , loss : 1.6129868030548096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 55.07 % , loss : 1.7824145555496216\n",
      "step : 55.51 % , loss : 1.716921091079712\n",
      "step : 55.95 % , loss : 1.6825144290924072\n",
      "step : 56.39 % , loss : 1.7136036157608032\n",
      "step : 56.83 % , loss : 1.6969246864318848\n",
      "step : 57.27 % , loss : 1.6115700006484985\n",
      "step : 57.71 % , loss : 1.6992357969284058\n",
      "step : 58.15 % , loss : 1.6133757829666138\n",
      "step : 58.59 % , loss : 1.6904641389846802\n",
      "step : 59.03 % , loss : 1.5912444591522217\n",
      "step : 59.47 % , loss : 1.582250952720642\n",
      "step : 59.91 % , loss : 1.6831692457199097\n",
      "step : 60.35 % , loss : 1.6673469543457031\n",
      "step : 60.79 % , loss : 1.6320255994796753\n",
      "step : 61.23 % , loss : 1.8425604104995728\n",
      "step : 61.67 % , loss : 1.525709867477417\n",
      "step : 62.11 % , loss : 1.723475694656372\n",
      "step : 62.56 % , loss : 1.7071369886398315\n",
      "step : 63.0 % , loss : 1.6712079048156738\n",
      "step : 63.44 % , loss : 1.6610499620437622\n",
      "step : 63.88 % , loss : 1.7917404174804688\n",
      "step : 64.32 % , loss : 1.8531543016433716\n",
      "step : 64.76 % , loss : 1.6413521766662598\n",
      "step : 65.2 % , loss : 1.5619428157806396\n",
      "step : 65.64 % , loss : 1.683088779449463\n",
      "step : 66.08 % , loss : 1.7084282636642456\n",
      "step : 66.52 % , loss : 1.8934876918792725\n",
      "step : 66.96 % , loss : 1.7022002935409546\n",
      "step : 67.4 % , loss : 1.6122766733169556\n",
      "step : 67.84 % , loss : 1.7491116523742676\n",
      "step : 68.28 % , loss : 1.6616345643997192\n",
      "step : 68.72 % , loss : 1.7109454870224\n",
      "step : 69.16 % , loss : 1.701918125152588\n",
      "step : 69.6 % , loss : 1.6385918855667114\n",
      "step : 70.04 % , loss : 1.5777244567871094\n",
      "step : 70.48 % , loss : 1.7913602590560913\n",
      "step : 70.93 % , loss : 1.6087450981140137\n",
      "step : 71.37 % , loss : 1.6489155292510986\n",
      "step : 71.81 % , loss : 1.6730844974517822\n",
      "step : 72.25 % , loss : 1.5100988149642944\n",
      "step : 72.69 % , loss : 1.7171986103057861\n",
      "step : 73.13 % , loss : 1.6200151443481445\n",
      "step : 73.57 % , loss : 1.5884034633636475\n",
      "step : 74.01 % , loss : 1.7612124681472778\n",
      "step : 74.45 % , loss : 1.704190969467163\n",
      "step : 74.89 % , loss : 1.5967742204666138\n",
      "step : 75.33 % , loss : 1.7413794994354248\n",
      "step : 75.77 % , loss : 1.5702495574951172\n",
      "step : 76.21 % , loss : 1.6622569561004639\n",
      "step : 76.65 % , loss : 1.6019458770751953\n",
      "step : 77.09 % , loss : 1.5712443590164185\n",
      "step : 77.53 % , loss : 1.706223726272583\n",
      "step : 77.97 % , loss : 1.6053446531295776\n",
      "step : 78.41 % , loss : 1.7451122999191284\n",
      "step : 78.85 % , loss : 1.6890619993209839\n",
      "step : 79.3 % , loss : 1.675121784210205\n",
      "step : 79.74 % , loss : 1.6955883502960205\n",
      "step : 80.18 % , loss : 1.7728227376937866\n",
      "step : 80.62 % , loss : 1.6683906316757202\n",
      "step : 81.06 % , loss : 1.6140440702438354\n",
      "step : 81.5 % , loss : 1.5405646562576294\n",
      "step : 81.94 % , loss : 1.6377261877059937\n",
      "step : 82.38 % , loss : 1.6248303651809692\n",
      "step : 82.82 % , loss : 1.6514889001846313\n",
      "step : 83.26 % , loss : 1.7254077196121216\n",
      "step : 83.7 % , loss : 1.712647795677185\n",
      "step : 84.14 % , loss : 1.5417424440383911\n",
      "step : 84.58 % , loss : 1.7634015083312988\n",
      "step : 85.02 % , loss : 1.588734745979309\n",
      "step : 85.46 % , loss : 1.7353912591934204\n",
      "step : 85.9 % , loss : 1.6373298168182373\n",
      "step : 86.34 % , loss : 1.6707640886306763\n",
      "step : 86.78 % , loss : 1.6573059558868408\n",
      "step : 87.22 % , loss : 1.6428533792495728\n",
      "step : 87.67 % , loss : 1.612426996231079\n",
      "step : 88.11 % , loss : 1.77956223487854\n",
      "step : 88.55 % , loss : 1.7404725551605225\n",
      "step : 88.99 % , loss : 1.6710023880004883\n",
      "step : 89.43 % , loss : 1.6718382835388184\n",
      "step : 89.87 % , loss : 1.603258490562439\n",
      "step : 90.31 % , loss : 1.6206156015396118\n",
      "step : 90.75 % , loss : 1.5799429416656494\n",
      "step : 91.19 % , loss : 1.6799085140228271\n",
      "step : 91.63 % , loss : 1.675051212310791\n",
      "step : 92.07 % , loss : 1.7031899690628052\n",
      "step : 92.51 % , loss : 1.5482314825057983\n",
      "step : 92.95 % , loss : 1.7538751363754272\n",
      "step : 93.39 % , loss : 1.6136621236801147\n",
      "step : 93.83 % , loss : 1.701154112815857\n",
      "step : 94.27 % , loss : 1.7925392389297485\n",
      "step : 94.71 % , loss : 1.694535493850708\n",
      "step : 95.15 % , loss : 1.7562451362609863\n",
      "step : 95.59 % , loss : 1.593177318572998\n",
      "step : 96.04 % , loss : 1.7176138162612915\n",
      "step : 96.48 % , loss : 1.5194199085235596\n",
      "step : 96.92 % , loss : 1.7326736450195312\n",
      "step : 97.36 % , loss : 1.6787171363830566\n",
      "step : 97.8 % , loss : 1.6911025047302246\n",
      "step : 98.24 % , loss : 1.612847924232483\n",
      "step : 98.68 % , loss : 1.5792295932769775\n",
      "step : 99.12 % , loss : 1.6397345066070557\n",
      "step : 99.56 % , loss : 1.6933305263519287\n",
      "Epoch: 09 | Time: 0m 8s\n",
      "\tTrain Loss: 1.671 | Train PPL: 5.315\n",
      "\tValidation Loss: 1.769 | Validation PPL: 5.867\n",
      "step : 0.0 % , loss : 1.5642855167388916\n",
      "step : 0.44 % , loss : 1.5659252405166626\n",
      "step : 0.88 % , loss : 1.4405200481414795\n",
      "step : 1.32 % , loss : 1.4615455865859985\n",
      "step : 1.76 % , loss : 1.5807487964630127\n",
      "step : 2.2 % , loss : 1.59967839717865\n",
      "step : 2.64 % , loss : 1.5572223663330078\n",
      "step : 3.08 % , loss : 1.4811843633651733\n",
      "step : 3.52 % , loss : 1.553720235824585\n",
      "step : 3.96 % , loss : 1.5304477214813232\n",
      "step : 4.41 % , loss : 1.5518871545791626\n",
      "step : 4.85 % , loss : 1.4705419540405273\n",
      "step : 5.29 % , loss : 1.3968820571899414\n",
      "step : 5.73 % , loss : 1.5822763442993164\n",
      "step : 6.17 % , loss : 1.4923312664031982\n",
      "step : 6.61 % , loss : 1.4935907125473022\n",
      "step : 7.05 % , loss : 1.586508870124817\n",
      "step : 7.49 % , loss : 1.5653421878814697\n",
      "step : 7.93 % , loss : 1.5371766090393066\n",
      "step : 8.37 % , loss : 1.704815149307251\n",
      "step : 8.81 % , loss : 1.4384725093841553\n",
      "step : 9.25 % , loss : 1.5149247646331787\n",
      "step : 9.69 % , loss : 1.3974820375442505\n",
      "step : 10.13 % , loss : 1.5178672075271606\n",
      "step : 10.57 % , loss : 1.4811369180679321\n",
      "step : 11.01 % , loss : 1.5135315656661987\n",
      "step : 11.45 % , loss : 1.606501579284668\n",
      "step : 11.89 % , loss : 1.4737727642059326\n",
      "step : 12.33 % , loss : 1.5040411949157715\n",
      "step : 12.78 % , loss : 1.5164649486541748\n",
      "step : 13.22 % , loss : 1.5422310829162598\n",
      "step : 13.66 % , loss : 1.6182090044021606\n",
      "step : 14.1 % , loss : 1.432803988456726\n",
      "step : 14.54 % , loss : 1.5026379823684692\n",
      "step : 14.98 % , loss : 1.5199044942855835\n",
      "step : 15.42 % , loss : 1.6268606185913086\n",
      "step : 15.86 % , loss : 1.4535009860992432\n",
      "step : 16.3 % , loss : 1.493180751800537\n",
      "step : 16.74 % , loss : 1.570016622543335\n",
      "step : 17.18 % , loss : 1.5292580127716064\n",
      "step : 17.62 % , loss : 1.5127627849578857\n",
      "step : 18.06 % , loss : 1.5618200302124023\n",
      "step : 18.5 % , loss : 1.607831358909607\n",
      "step : 18.94 % , loss : 1.5704644918441772\n",
      "step : 19.38 % , loss : 1.42130708694458\n",
      "step : 19.82 % , loss : 1.6277803182601929\n",
      "step : 20.26 % , loss : 1.5692434310913086\n",
      "step : 20.7 % , loss : 1.5494593381881714\n",
      "step : 21.15 % , loss : 1.4897531270980835\n",
      "step : 21.59 % , loss : 1.5282104015350342\n",
      "step : 22.03 % , loss : 1.5165174007415771\n",
      "step : 22.47 % , loss : 1.5131226778030396\n",
      "step : 22.91 % , loss : 1.502069115638733\n",
      "step : 23.35 % , loss : 1.7193498611450195\n",
      "step : 23.79 % , loss : 1.5214670896530151\n",
      "step : 24.23 % , loss : 1.5446652173995972\n",
      "step : 24.67 % , loss : 1.6486961841583252\n",
      "step : 25.11 % , loss : 1.577736258506775\n",
      "step : 25.55 % , loss : 1.502847671508789\n",
      "step : 25.99 % , loss : 1.5475043058395386\n",
      "step : 26.43 % , loss : 1.5766133069992065\n",
      "step : 26.87 % , loss : 1.6638941764831543\n",
      "step : 27.31 % , loss : 1.5426920652389526\n",
      "step : 27.75 % , loss : 1.5810160636901855\n",
      "step : 28.19 % , loss : 1.4540170431137085\n",
      "step : 28.63 % , loss : 1.669519305229187\n",
      "step : 29.07 % , loss : 1.4373525381088257\n",
      "step : 29.52 % , loss : 1.6100760698318481\n",
      "step : 29.96 % , loss : 1.6625287532806396\n",
      "step : 30.4 % , loss : 1.5412046909332275\n",
      "step : 30.84 % , loss : 1.451686978340149\n",
      "step : 31.28 % , loss : 1.5951216220855713\n",
      "step : 31.72 % , loss : 1.4586442708969116\n",
      "step : 32.16 % , loss : 1.4735980033874512\n",
      "step : 32.6 % , loss : 1.478620171546936\n",
      "step : 33.04 % , loss : 1.5256692171096802\n",
      "step : 33.48 % , loss : 1.4746036529541016\n",
      "step : 33.92 % , loss : 1.6179760694503784\n",
      "step : 34.36 % , loss : 1.544177770614624\n",
      "step : 34.8 % , loss : 1.6014999151229858\n",
      "step : 35.24 % , loss : 1.5402989387512207\n",
      "step : 35.68 % , loss : 1.4986745119094849\n",
      "step : 36.12 % , loss : 1.4843732118606567\n",
      "step : 36.56 % , loss : 1.5239282846450806\n",
      "step : 37.0 % , loss : 1.5099012851715088\n",
      "step : 37.44 % , loss : 1.6656404733657837\n",
      "step : 37.89 % , loss : 1.5629487037658691\n",
      "step : 38.33 % , loss : 1.5378739833831787\n",
      "step : 38.77 % , loss : 1.5792616605758667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 39.21 % , loss : 1.6745364665985107\n",
      "step : 39.65 % , loss : 1.496378779411316\n",
      "step : 40.09 % , loss : 1.615454077720642\n",
      "step : 40.53 % , loss : 1.5667877197265625\n",
      "step : 40.97 % , loss : 1.441941738128662\n",
      "step : 41.41 % , loss : 1.4123860597610474\n",
      "step : 41.85 % , loss : 1.574682354927063\n",
      "step : 42.29 % , loss : 1.5712674856185913\n",
      "step : 42.73 % , loss : 1.4710636138916016\n",
      "step : 43.17 % , loss : 1.4264535903930664\n",
      "step : 43.61 % , loss : 1.5095493793487549\n",
      "step : 44.05 % , loss : 1.4539296627044678\n",
      "step : 44.49 % , loss : 1.7464901208877563\n",
      "step : 44.93 % , loss : 1.5833818912506104\n",
      "step : 45.37 % , loss : 1.596808671951294\n",
      "step : 45.81 % , loss : 1.5507242679595947\n",
      "step : 46.26 % , loss : 1.6513992547988892\n",
      "step : 46.7 % , loss : 1.5212410688400269\n",
      "step : 47.14 % , loss : 1.4610700607299805\n",
      "step : 47.58 % , loss : 1.5668747425079346\n",
      "step : 48.02 % , loss : 1.3979589939117432\n",
      "step : 48.46 % , loss : 1.6900055408477783\n",
      "step : 48.9 % , loss : 1.5669993162155151\n",
      "step : 49.34 % , loss : 1.5082380771636963\n",
      "step : 49.78 % , loss : 1.67727530002594\n",
      "step : 50.22 % , loss : 1.5124151706695557\n",
      "step : 50.66 % , loss : 1.5756609439849854\n",
      "step : 51.1 % , loss : 1.57016921043396\n",
      "step : 51.54 % , loss : 1.5785459280014038\n",
      "step : 51.98 % , loss : 1.6583386659622192\n",
      "step : 52.42 % , loss : 1.5461351871490479\n",
      "step : 52.86 % , loss : 1.5692439079284668\n",
      "step : 53.3 % , loss : 1.5835076570510864\n",
      "step : 53.74 % , loss : 1.5441888570785522\n",
      "step : 54.19 % , loss : 1.6319624185562134\n",
      "step : 54.63 % , loss : 1.4685033559799194\n",
      "step : 55.07 % , loss : 1.6526899337768555\n",
      "step : 55.51 % , loss : 1.6293056011199951\n",
      "step : 55.95 % , loss : 1.6243820190429688\n",
      "step : 56.39 % , loss : 1.5271331071853638\n",
      "step : 56.83 % , loss : 1.5071513652801514\n",
      "step : 57.27 % , loss : 1.5838892459869385\n",
      "step : 57.71 % , loss : 1.4811996221542358\n",
      "step : 58.15 % , loss : 1.6489635705947876\n",
      "step : 58.59 % , loss : 1.6386643648147583\n",
      "step : 59.03 % , loss : 1.6630172729492188\n",
      "step : 59.47 % , loss : 1.7087303400039673\n",
      "step : 59.91 % , loss : 1.606383204460144\n",
      "step : 60.35 % , loss : 1.461744785308838\n",
      "step : 60.79 % , loss : 1.6267285346984863\n",
      "step : 61.23 % , loss : 1.601090431213379\n",
      "step : 61.67 % , loss : 1.6912260055541992\n",
      "step : 62.11 % , loss : 1.6950520277023315\n",
      "step : 62.56 % , loss : 1.5316563844680786\n",
      "step : 63.0 % , loss : 1.5542521476745605\n",
      "step : 63.44 % , loss : 1.6450040340423584\n",
      "step : 63.88 % , loss : 1.6739492416381836\n",
      "step : 64.32 % , loss : 1.5921083688735962\n",
      "step : 64.76 % , loss : 1.5808135271072388\n",
      "step : 65.2 % , loss : 1.6631680727005005\n",
      "step : 65.64 % , loss : 1.675107717514038\n",
      "step : 66.08 % , loss : 1.5744481086730957\n",
      "step : 66.52 % , loss : 1.5236375331878662\n",
      "step : 66.96 % , loss : 1.6086032390594482\n",
      "step : 67.4 % , loss : 1.5525459051132202\n",
      "step : 67.84 % , loss : 1.391104817390442\n",
      "step : 68.28 % , loss : 1.5061819553375244\n",
      "step : 68.72 % , loss : 1.4108257293701172\n",
      "step : 69.16 % , loss : 1.6290037631988525\n",
      "step : 69.6 % , loss : 1.4342129230499268\n",
      "step : 70.04 % , loss : 1.5569045543670654\n",
      "step : 70.48 % , loss : 1.4844202995300293\n",
      "step : 70.93 % , loss : 1.6179041862487793\n",
      "step : 71.37 % , loss : 1.5273094177246094\n",
      "step : 71.81 % , loss : 1.577559232711792\n",
      "step : 72.25 % , loss : 1.5498088598251343\n",
      "step : 72.69 % , loss : 1.471229076385498\n",
      "step : 73.13 % , loss : 1.4492148160934448\n",
      "step : 73.57 % , loss : 1.6068577766418457\n",
      "step : 74.01 % , loss : 1.5605123043060303\n",
      "step : 74.45 % , loss : 1.6444718837738037\n",
      "step : 74.89 % , loss : 1.5323290824890137\n",
      "step : 75.33 % , loss : 1.5482851266860962\n",
      "step : 75.77 % , loss : 1.5111030340194702\n",
      "step : 76.21 % , loss : 1.5912870168685913\n",
      "step : 76.65 % , loss : 1.6320947408676147\n",
      "step : 77.09 % , loss : 1.7231308221817017\n",
      "step : 77.53 % , loss : 1.5598318576812744\n",
      "step : 77.97 % , loss : 1.6678978204727173\n",
      "step : 78.41 % , loss : 1.6845591068267822\n",
      "step : 78.85 % , loss : 1.5523203611373901\n",
      "step : 79.3 % , loss : 1.4974918365478516\n",
      "step : 79.74 % , loss : 1.513319969177246\n",
      "step : 80.18 % , loss : 1.4808372259140015\n",
      "step : 80.62 % , loss : 1.7185664176940918\n",
      "step : 81.06 % , loss : 1.6084221601486206\n",
      "step : 81.5 % , loss : 1.7673643827438354\n",
      "step : 81.94 % , loss : 1.566428303718567\n",
      "step : 82.38 % , loss : 1.456956386566162\n",
      "step : 82.82 % , loss : 1.5652540922164917\n",
      "step : 83.26 % , loss : 1.6629538536071777\n",
      "step : 83.7 % , loss : 1.4884591102600098\n",
      "step : 84.14 % , loss : 1.8004978895187378\n",
      "step : 84.58 % , loss : 1.65037202835083\n",
      "step : 85.02 % , loss : 1.656461477279663\n",
      "step : 85.46 % , loss : 1.4265825748443604\n",
      "step : 85.9 % , loss : 1.537992000579834\n",
      "step : 86.34 % , loss : 1.7378730773925781\n",
      "step : 86.78 % , loss : 1.652853012084961\n",
      "step : 87.22 % , loss : 1.582206130027771\n",
      "step : 87.67 % , loss : 1.410775065422058\n",
      "step : 88.11 % , loss : 1.5899956226348877\n",
      "step : 88.55 % , loss : 1.6362248659133911\n",
      "step : 88.99 % , loss : 1.660241961479187\n",
      "step : 89.43 % , loss : 1.5962601900100708\n",
      "step : 89.87 % , loss : 1.504471778869629\n",
      "step : 90.31 % , loss : 1.6539722681045532\n",
      "step : 90.75 % , loss : 1.7003729343414307\n",
      "step : 91.19 % , loss : 1.7939982414245605\n",
      "step : 91.63 % , loss : 1.6716032028198242\n",
      "step : 92.07 % , loss : 1.6253509521484375\n",
      "step : 92.51 % , loss : 1.627729892730713\n",
      "step : 92.95 % , loss : 1.7039631605148315\n",
      "step : 93.39 % , loss : 1.5634297132492065\n",
      "step : 93.83 % , loss : 1.7406474351882935\n",
      "step : 94.27 % , loss : 1.6030603647232056\n",
      "step : 94.71 % , loss : 1.662866234779358\n",
      "step : 95.15 % , loss : 1.543104648590088\n",
      "step : 95.59 % , loss : 1.5635489225387573\n",
      "step : 96.04 % , loss : 1.5046460628509521\n",
      "step : 96.48 % , loss : 1.7033534049987793\n",
      "step : 96.92 % , loss : 1.6230895519256592\n",
      "step : 97.36 % , loss : 1.615143895149231\n",
      "step : 97.8 % , loss : 1.4922775030136108\n",
      "step : 98.24 % , loss : 1.592081069946289\n",
      "step : 98.68 % , loss : 1.567561149597168\n",
      "step : 99.12 % , loss : 1.527288556098938\n",
      "step : 99.56 % , loss : 1.539542555809021\n",
      "Epoch: 10 | Time: 0m 8s\n",
      "\tTrain Loss: 1.565 | Train PPL: 4.781\n",
      "\tValidation Loss: 1.723 | Validation PPL: 5.601\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ad07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c75bf735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,987,141 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d0422efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.933 | Test PPL: 6.910\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cce79494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역(translation) 함수\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50, logging=True):\n",
    "    model.eval() # 평가 모드\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # 처음에  토큰, 마지막에  토큰 붙이기\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    if logging:\n",
    "        print(f\"전체 소스 토큰: {tokens}\")\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    if logging:\n",
    "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    # 소스 문장에 따른 마스크 생성\n",
    "    src_mask = model.make_pad_mask(src_tensor,src_tensor)\n",
    "\n",
    "    # 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # 처음에는  토큰 하나만 가지고 있도록 하기 --> start token으로만 시작해야함 타겟문장이 뭔지 모르기때문에.\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len): # 출력하고 싶은 문장의 최대길이\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        # 출력 문장에 따른 마스크 생성 , 여기서는 trg_tensor의 길이를 모르기때문에 \n",
    "        trg_mask = model.make_pad_mask(trg_tensor, trg_tensor) * model.make_no_peak_mask(trg_tensor, trg_tensor)\n",
    "        src_trg_mask = model.make_pad_mask(trg_tensor, src_tensor)\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(trg_tensor, enc_src, trg_mask, src_trg_mask)\n",
    "\n",
    "        # 출력 문장에서 가장 마지막 단어만 사용 # trg가 한단어라면 한단어가 결과로 나옴. 4단어라면 4단어 결과가나옴.\n",
    "        pred_token = output.argmax(-1)[:,-1].item() # output -> [Batch, trg_len - 1, output_dim] -> [Batch, trg_len - 1]\n",
    "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
    "\n",
    "        # end token을 만나는 순간 끝\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    # 첫 번째 는 제외하고 출력 문장 반환\n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0981af39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소스 문장: ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']\n",
      "타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n",
      "전체 소스 토큰: ['<sos>', 'eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.', '<eos>']\n",
      "소스 문장 인덱스: [2, 8, 364, 10, 134, 70, 624, 565, 19, 780, 200, 20, 88, 4, 3]\n",
      "모델 출력 결과: a mother and her daughter are enjoying a beautiful day outdoors . <eos>\n"
     ]
    }
   ],
   "source": [
    "example_idx = 10\n",
    "\n",
    "src = vars(test_dataset.examples[example_idx])['src']\n",
    "trg = vars(test_dataset.examples[example_idx])['trg']\n",
    "\n",
    "print(f'소스 문장: {src}')\n",
    "print(f'타겟 문장: {trg}')\n",
    "\n",
    "translation = translate_sentence(src, SRC, TRG, model, device, logging=True)\n",
    "\n",
    "print(\"모델 출력 결과:\", \" \".join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fa18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca758b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 실제로 길이가 다른 문장을 이용할때 사용하는 방법 예시\n",
    "\n",
    "# train_src_data = [\n",
    "# [62, 13, 47, 39, 78, 33, 56, 13],\n",
    "# [60, 96, 51, 32, 90],\n",
    "# [35, 45, 48, 65, 91, 99, 92, 10,  3, 21],\n",
    "# [66, 88, 98, 47],\n",
    "# [77, 65, 51, 77, 19, 15, 35, 19, 23]\n",
    "# ]\n",
    "\n",
    "# train_trg_data = [\n",
    "# [1, 33, 11, 49, 10],\n",
    "# [1, 88, 34,  5, 29, 99, 45, 11, 25],\n",
    "# [1, 67, 25, 15, 90, 54,  4, 92, 10, 46, 20, 88, 19],\n",
    "# [1, 16, 58, 91, 47, 12,  5,  8],\n",
    "# [1, 71, 63, 62,  7,  9, 11, 55, 91, 32, 48]\n",
    "# ]\n",
    "\n",
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self,src_data,trg_data):\n",
    "#         super().__init__()\n",
    "#         self.src_data = src_data\n",
    "#         self.trg_data = trg_data\n",
    "\n",
    "#     def __getitem__(self,index):\n",
    "#         return torch.LongTensor(self.src_data[index]),torch.LongTensor(self.trg_data[index])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.src_data)\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     src_batch, tgt_batch = [], []\n",
    "#     for src_sample, tgt_sample in batch:\n",
    "#         src_batch.append(src_sample)\n",
    "#         tgt_batch.append(tgt_sample)\n",
    "\n",
    "#     src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0)\n",
    "#     tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=0)\n",
    "#     return src_batch.transpose(0,1), tgt_batch.transpose(0,1)\n",
    "\n",
    "# dataset = CustomDataset(src_data,trg_data)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset,batch_size=2,shuffle=True,collate_fn=collate_fn)\n",
    "# src,trg = next(iter(train_loader))\n",
    "# print(src)\n",
    "# print(src.size()) # Batch,Length\n",
    "# print(trg)\n",
    "# print(trg.size()) # Batch,Length\n",
    "# '''\n",
    "# tensor([[60, 96, 51, 32, 90,  0,  0,  0],\n",
    "#         [62, 13, 47, 39, 78, 33, 56, 13]])\n",
    "# torch.Size([2, 8])\n",
    "# tensor([[ 1, 88, 34,  5, 29, 99, 45, 11, 25],\n",
    "#         [ 1, 33, 11, 49, 10,  0,  0,  0,  0]])\n",
    "# torch.Size([2, 9])\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6a7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
